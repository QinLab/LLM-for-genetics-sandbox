{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/qxy699/scGPTenv/lib/python3.10/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/qxy699/scGPTenv/lib/python3.10/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/qxy699/scGPTenv/lib/python3.10/site-packages/scanpy/_settings.py:488: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from biomart import BiomartServer\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=\"fly\",\n",
    "    do_train=False,\n",
    "    load_model=\"./model.pt\",\n",
    "    mask_ratio=0.0,\n",
    "    epochs=10,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=1,\n",
    "    layer_size=512,\n",
    "    nlayers=12,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=8,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = hyperparameter_defaults[\"mask_ratio\"]\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "include_zero_gene = hyperparameter_defaults[\"include_zero_gene\"]  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 4716\n",
    "n_bins = hyperparameter_defaults[\"n_bins\"]\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = hyperparameter_defaults[\"MVC\"]  # Masked value prediction for cell embedding\n",
    "ECS = hyperparameter_defaults[\"ecs_thres\"] > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = hyperparameter_defaults[\"ecs_thres\"]\n",
    "dab_weight = hyperparameter_defaults[\"dab_weight\"]\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = hyperparameter_defaults[\"lr\"]  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = hyperparameter_defaults[\"batch_size\"]\n",
    "eval_batch_size = hyperparameter_defaults[\"batch_size\"]\n",
    "epochs = hyperparameter_defaults[\"epochs\"]\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = hyperparameter_defaults[\"fast_transformer\"]\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = hyperparameter_defaults[\"layer_size\"]  # embedding dimension\n",
    "d_hid = hyperparameter_defaults[\"layer_size\"]  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = hyperparameter_defaults[\"nlayers\"]  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = hyperparameter_defaults[\"nhead\"]  # number of heads in nn.MultiheadAttention\n",
    "dropout = hyperparameter_defaults[\"dropout\"]  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = hyperparameter_defaults[\"save_eval_interval\"]  # epochs\n",
    "do_eval_scib_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = Path(\"/home/phv028r/scGPT-aging/raw-data/fly\")\n",
    "adata = sc.read(\"./adata_headBody_S_v1.0.h5ad\")\n",
    "adata.obs[\"age\"] = adata.obs[\"age\"].astype(\"category\")\n",
    "adata.var[\"gene_name\"] = adata.var.index\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "data_is_raw = False\n",
    "filter_gene_by_counts = False\n",
    "\n",
    "ensembl_server = BiomartServer(\"http://www.ensembl.org/biomart\")\n",
    "ensembl_dataset = ensembl_server.datasets['dmelanogaster_gene_ensembl']\n",
    "ensembl_query = ensembl_dataset.search({\n",
    "#         \"filters\": {\n",
    "#             \"external_gene_name\": adata.var[\"gene_name\"].tolist(),\n",
    "#         },\n",
    "    'attributes': [\n",
    "#             'flybase_gene_id',        # Fly gene ID\n",
    "        'external_gene_name',     # Fly gene name\n",
    "        'hsapiens_homolog_ensembl_gene', # Human gene ID\n",
    "        'hsapiens_homolog_associated_gene_name' # Human gene name\n",
    "    ]\n",
    "})\n",
    "\n",
    "ensembl_query_text = ensembl_query.text\n",
    "ensembl_query_lines = ensembl_query_text.strip().split(\"\\n\")\n",
    "ensembl_query_columns = [\"fly_gene\", \"human_ensembl_id\", \"human_gene\"]\n",
    "ensembl_query_text = [line.split(\"\\t\") for line in ensembl_query_lines[1:]]\n",
    "ensembl_query_df = pd.DataFrame(ensembl_query_text, columns=ensembl_query_columns)\n",
    "ensembl_query_df['human_gene'].replace('', pd.NA, inplace=True)\n",
    "filtered_ensembl_query_df = ensembl_query_df.dropna(subset=[\"human_gene\"])\n",
    "map_fly_to_human_dict = pd.Series(filtered_ensembl_query_df.human_gene.values, index=filtered_ensembl_query_df.fly_gene).to_dict()\n",
    "reverse_mapping = dict()\n",
    "for fly_gene, human_gene in map_fly_to_human_dict.items():\n",
    "    reverse_mapping[human_gene] = fly_gene\n",
    "reverse_mapping.pop(\"AGO2\")\n",
    "reverse_mapping.pop(\"PCNA\")\n",
    "reverse_mapping.pop(\"SPR\")\n",
    "reverse_mapping.pop(\"PGAP2\")\n",
    "reverse_mapping.pop(\"MED22\")\n",
    "one_to_one_mapping = {v: k for k, v in reverse_mapping.items()}\n",
    "new_index = []\n",
    "for gene in adata.var.index:\n",
    "    if gene in one_to_one_mapping:\n",
    "        new_index.append(one_to_one_mapping[gene])\n",
    "    else:\n",
    "        new_index.append(gene)\n",
    "adata.var.index = pd.Index(new_index)\n",
    "\n",
    "\n",
    "# make the batch category column\n",
    "age_id_labels = adata.obs[\"age\"].astype(\"category\").cat.codes.values\n",
    "ages = adata.obs[\"age\"].unique()\n",
    "num_types = len(np.unique(age_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"age\"].astype(\"category\").cat.categories))\n",
    "adata.obs[\"age_id\"] = age_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "# randomly sample cells\n",
    "sampled_indices = []\n",
    "for age_category in ages:\n",
    "    age_group = adata.obs[adata.obs['age'] == age_category]\n",
    "    sampled_group = age_group.sample(n=250, random_state=42)\n",
    "    sampled_indices.extend(sampled_group.index)\n",
    "adata = adata[sampled_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 4715 × 1000\n",
       "    obs: 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'gene_name', 'id_in_vocab'\n",
       "    var: 'tissue', 'sex', 'age', 'sex_age', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'log1p_n_genes_by_counts', 'log1p_total_counts', 'log1p_total_counts_mt', 'dataset', 'fca_annotation', 'afca_annotation', 'afca_annotation_broad', 'age_id'\n",
       "    uns: 'age_colors', 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'sex_colors', 'tissue_colors', 'tsne', 'umap'\n",
       "    varm: 'X_pca', 'X_tsne', 'X_umap', 'bin_edges'\n",
       "    layers: 'X_binned'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_csvs(\"./adata.csv\", skip_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              TCATATCTCACCGGGT-1_AFCA_female_body_30_S4\n",
       "1                TCCGAAAGTAGGCTGA-1_AFCA_male_body_30_S5\n",
       "2                CCGAACGAGGCCCACT-1_AFCA_male_body_30_S3\n",
       "3              CGTGATATCCCTAGGG-1_AFCA_female_body_30_S2\n",
       "4              CAGATTGAGAATTGTG-1_AFCA_female_body_30_S5\n",
       "                             ...                        \n",
       "995    GTGATGTTCGAGTGGA-f8548b44__FCA27_Female_body_a...\n",
       "996    GTCTTTAAGGTTGTTC-f7dc3ba8__FCA13_Female_head_a...\n",
       "997      ACGTCCTAGTAGATCA-d541ae4e__FCA1_MaleFemale_Head\n",
       "998    TTTCACATCCGTCCTA-f8548b44__FCA27_Female_body_a...\n",
       "999    GGCTTTCAGGCATGGT-f8438c5e__FCA25_Female_body_a...\n",
       "Name: Unnamed: 0, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_gene = pd.read_csv(\"/home/qxy699/scGPT/adata/obs.csv\")\n",
    "dataset_gene[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1000 × 4715\n",
       "    obs: 'tissue', 'sex', 'age', 'sex_age', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'log1p_n_genes_by_counts', 'log1p_total_counts', 'log1p_total_counts_mt', 'dataset', 'fca_annotation', 'afca_annotation', 'afca_annotation_broad', 'age_id'\n",
       "    var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'gene_name', 'id_in_vocab'\n",
       "    uns: 'age_colors', 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'sex_colors', 'tissue_colors', 'tsne', 'umap'\n",
       "    obsm: 'X_pca', 'X_tsne', 'X_umap', 'bin_edges'\n",
       "    layers: 'X_binned'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match 4715/15992 genes in vocabulary of size 60697.\n"
     ]
    }
   ],
   "source": [
    "# model_dir = Path(hyperparameter_defaults[\"load_model\"])\n",
    "model_file = \"./model.pt\"\n",
    "vocab_file = \"./vocab.json\"\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s)\n",
    "\n",
    "adata.var[\"id_in_vocab\"] = [\n",
    "    1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "]\n",
    "gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "print(\n",
    "    f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "    f\"in vocabulary of size {len(vocab)}.\"\n",
    ")\n",
    "\n",
    "adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.sparse import issparse\n",
    "import scanpy as sc\n",
    "from scanpy.get import _get_obs_rep, _set_obs_rep\n",
    "from anndata import AnnData\n",
    "\n",
    "from scgpt import logger\n",
    "\n",
    "\n",
    "class Preprocessor_Edit:\n",
    "    \"\"\"\n",
    "    Prepare data into training, valid and test split. Normalize raw expression\n",
    "    values, binning or using other transform into the preset model input format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_key: Optional[str] = None,\n",
    "        filter_gene_by_counts: Union[int, bool] = False,\n",
    "        filter_cell_by_counts: Union[int, bool] = False,\n",
    "        normalize_total: Union[float, bool] = 1e4,\n",
    "        result_normed_key: Optional[str] = \"X_normed\",\n",
    "        log1p: bool = False,\n",
    "        result_log1p_key: str = \"X_log1p\",\n",
    "        subset_hvg: Union[int, bool] = False,\n",
    "        hvg_use_key: Optional[str] = None,\n",
    "        hvg_flavor: str = \"seurat_v3\",\n",
    "        binning: Optional[int] = None,\n",
    "        result_binned_key: str = \"X_binned\",\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Set up the preprocessor, use the args to config the workflow steps.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        use_key (:class:`str`, optional):\n",
    "            The key of :class:`~anndata.AnnData` to use for preprocessing.\n",
    "        filter_gene_by_counts (:class:`int` or :class:`bool`, default: ``False``):\n",
    "            Whther to filter genes by counts, if :class:`int`, filter genes with counts\n",
    "        filter_cell_by_counts (:class:`int` or :class:`bool`, default: ``False``):\n",
    "            Whther to filter cells by counts, if :class:`int`, filter cells with counts\n",
    "        normalize_total (:class:`float` or :class:`bool`, default: ``1e4``):\n",
    "            Whether to normalize the total counts of each cell to a specific value.\n",
    "        result_normed_key (:class:`str`, default: ``\"X_normed\"``):\n",
    "            The key of :class:`~anndata.AnnData` to store the normalized data. If\n",
    "            :class:`None`, will use normed data to replce the :attr:`use_key`.\n",
    "        log1p (:class:`bool`, default: ``True``):\n",
    "            Whether to apply log1p transform to the normalized data.\n",
    "        result_log1p_key (:class:`str`, default: ``\"X_log1p\"``):\n",
    "            The key of :class:`~anndata.AnnData` to store the log1p transformed data.\n",
    "        subset_hvg (:class:`int` or :class:`bool`, default: ``False``):\n",
    "            Whether to subset highly variable genes.\n",
    "        hvg_use_key (:class:`str`, optional):\n",
    "            The key of :class:`~anndata.AnnData` to use for calculating highly variable\n",
    "            genes. If :class:`None`, will use :attr:`adata.X`.\n",
    "        hvg_flavor (:class:`str`, default: ``\"seurat_v3\"``):\n",
    "            The flavor of highly variable genes selection. See\n",
    "            :func:`scanpy.pp.highly_variable_genes` for more details.\n",
    "        binning (:class:`int`, optional):\n",
    "            Whether to bin the data into discrete values of number of bins provided.\n",
    "        result_binned_key (:class:`str`, default: ``\"X_binned\"``):\n",
    "            The key of :class:`~anndata.AnnData` to store the binned data.\n",
    "        \"\"\"\n",
    "        self.use_key = use_key\n",
    "        self.filter_gene_by_counts = filter_gene_by_counts\n",
    "        self.filter_cell_by_counts = filter_cell_by_counts\n",
    "        self.normalize_total = normalize_total\n",
    "        self.result_normed_key = result_normed_key\n",
    "        self.log1p = log1p\n",
    "        self.result_log1p_key = result_log1p_key\n",
    "        self.subset_hvg = subset_hvg\n",
    "        self.hvg_use_key = hvg_use_key\n",
    "        self.hvg_flavor = hvg_flavor\n",
    "        self.binning = binning\n",
    "        self.result_binned_key = result_binned_key\n",
    "\n",
    "    def __call__(self, adata: AnnData, batch_key: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        format controls the different input value wrapping, including categorical\n",
    "        binned style, fixed-sum normalized counts, log1p fixed-sum normalized counts, etc.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        adata (:class:`AnnData`):\n",
    "            The :class:`AnnData` object to preprocess.\n",
    "        batch_key (:class:`str`, optional):\n",
    "            The key of :class:`AnnData.obs` to use for batch information. This arg\n",
    "            is used in the highly variable gene selection step.\n",
    "        \"\"\"\n",
    "        key_to_process = self.use_key\n",
    "        # preliminary checks, will use later\n",
    "        if key_to_process == \"X\":\n",
    "            key_to_process = None  # the following scanpy apis use arg None to use X\n",
    "        is_logged = self.check_logged(adata, obs_key=key_to_process)\n",
    "\n",
    "        # step 1: filter genes\n",
    "        if self.filter_gene_by_counts:\n",
    "            logger.info(\"Filtering genes by counts ...\")\n",
    "            sc.pp.filter_genes(\n",
    "                adata,\n",
    "                min_counts=self.filter_gene_by_counts\n",
    "                if isinstance(self.filter_gene_by_counts, int)\n",
    "                else None,\n",
    "            )\n",
    "\n",
    "        # step 2: filter cells\n",
    "        if (\n",
    "            isinstance(self.filter_cell_by_counts, int)\n",
    "            and self.filter_cell_by_counts > 0\n",
    "        ):\n",
    "            logger.info(\"Filtering cells by counts ...\")\n",
    "            sc.pp.filter_cells(\n",
    "                adata,\n",
    "                min_counts=self.filter_cell_by_counts\n",
    "                if isinstance(self.filter_cell_by_counts, int)\n",
    "                else None,\n",
    "            )\n",
    "\n",
    "        # step 3: normalize total\n",
    "        if self.normalize_total:\n",
    "            logger.info(\"Normalizing total counts ...\")\n",
    "            normed_ = sc.pp.normalize_total(\n",
    "                adata,\n",
    "                target_sum=self.normalize_total\n",
    "                if isinstance(self.normalize_total, float)\n",
    "                else None,\n",
    "                layer=key_to_process,\n",
    "                inplace=False,\n",
    "            )[\"X\"]\n",
    "            key_to_process = self.result_normed_key or key_to_process\n",
    "            _set_obs_rep(adata, normed_, layer=key_to_process)\n",
    "\n",
    "        # step 4: log1p\n",
    "        if self.log1p:\n",
    "            logger.info(\"Log1p transforming ...\")\n",
    "            if is_logged:\n",
    "                logger.warning(\n",
    "                    \"The input data seems to be already log1p transformed. \"\n",
    "                    \"Set `log1p=False` to avoid double log1p transform.\"\n",
    "                )\n",
    "            if self.result_log1p_key:\n",
    "                _set_obs_rep(\n",
    "                    adata,\n",
    "                    _get_obs_rep(adata, layer=key_to_process),\n",
    "                    layer=self.result_log1p_key,\n",
    "                )\n",
    "                key_to_process = self.result_log1p_key\n",
    "            sc.pp.log1p(adata, layer=key_to_process)\n",
    "\n",
    "        # step 5: subset hvg\n",
    "        if self.subset_hvg:\n",
    "            logger.info(\"Subsetting highly variable genes ...\")\n",
    "            if batch_key is None:\n",
    "                logger.warning(\n",
    "                    \"No batch_key is provided, will use all cells for HVG selection.\"\n",
    "                )\n",
    "            sc.pp.highly_variable_genes(\n",
    "                adata,\n",
    "                layer=self.hvg_use_key,\n",
    "                n_top_genes=self.subset_hvg\n",
    "                if isinstance(self.subset_hvg, int)\n",
    "                else None,\n",
    "                batch_key=batch_key,\n",
    "                flavor=self.hvg_flavor,\n",
    "                subset=True,\n",
    "            )\n",
    "\n",
    "        # step 6: binning\n",
    "        if self.binning:\n",
    "            logger.info(\"Binning data ...\")\n",
    "            if not isinstance(self.binning, int):\n",
    "                raise ValueError(\n",
    "                    \"Binning arg must be an integer, but got {}.\".format(self.binning)\n",
    "                )\n",
    "            n_bins = self.binning  # NOTE: the first bin is always a spectial for zero\n",
    "            binned_rows = []\n",
    "            bin_edges = []\n",
    "            layer_data = _get_obs_rep(adata, layer=key_to_process)\n",
    "            layer_data = layer_data.toarray() if issparse(layer_data) else layer_data\n",
    "            if layer_data.min() < 0:\n",
    "                raise ValueError(\n",
    "                    f\"Assuming non-negative data, but got min value {layer_data.min()}.\"\n",
    "                )\n",
    "            for row in layer_data:\n",
    "                if row.max() == 0:\n",
    "                    logger.warning(\n",
    "                        \"The input data contains all zero rows. Please make sure \"\n",
    "                        \"this is expected. You can use the `filter_cell_by_counts` \"\n",
    "                        \"arg to filter out all zero rows.\"\n",
    "                    )\n",
    "                    binned_rows.append(np.zeros_like(row, dtype=np.int64))\n",
    "                    bin_edges.append(np.array([0] * n_bins))\n",
    "                    continue\n",
    "                non_zero_ids = row.nonzero()\n",
    "                non_zero_row = row[non_zero_ids]\n",
    "                bins = np.quantile(non_zero_row, np.linspace(0, 1, n_bins - 1))\n",
    "                # bins = np.sort(np.unique(bins))\n",
    "                # NOTE: comment this line for now, since this will make the each category\n",
    "                # has different relative meaning across datasets\n",
    "                non_zero_digits = _digitize(non_zero_row, bins)\n",
    "                assert non_zero_digits.min() >= 1\n",
    "                assert non_zero_digits.max() <= n_bins - 1\n",
    "                binned_row = np.zeros_like(row, dtype=np.int64)\n",
    "                binned_row[non_zero_ids] = non_zero_digits\n",
    "                binned_rows.append(binned_row)\n",
    "                bin_edges.append(np.concatenate([[0], bins]))\n",
    "            adata.layers[self.result_binned_key] = np.stack(binned_rows)\n",
    "            adata.obsm[\"bin_edges\"] = np.stack(bin_edges)\n",
    "\n",
    "    def check_logged(self, adata: AnnData, obs_key: Optional[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the data is already log1p transformed.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        adata (:class:`AnnData`):\n",
    "            The :class:`AnnData` object to preprocess.\n",
    "        obs_key (:class:`str`, optional):\n",
    "            The key of :class:`AnnData.obs` to use for batch information. This arg\n",
    "            is used in the highly variable gene selection step.\n",
    "        \"\"\"\n",
    "        data = _get_obs_rep(adata, layer=obs_key)\n",
    "        max_, min_ = data.max(), data.min()\n",
    "        if max_ > 30:\n",
    "            return False\n",
    "        if min_ < 0:\n",
    "            return False\n",
    "\n",
    "        non_zero_min = data[data > 0].min()\n",
    "        if non_zero_min >= 1:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "def _digitize(x: np.ndarray, bins: np.ndarray, side=\"both\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Digitize the data into bins. This method spreads data uniformly when bins\n",
    "    have same values.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    x (:class:`np.ndarray`):\n",
    "        The data to digitize.\n",
    "    bins (:class:`np.ndarray`):\n",
    "        The bins to use for digitization, in increasing order.\n",
    "    side (:class:`str`, optional):\n",
    "        The side to use for digitization. If \"one\", the left side is used. If\n",
    "        \"both\", the left and right side are used. Default to \"one\".\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    :class:`np.ndarray`:\n",
    "        The digitized data.\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1 and bins.ndim == 1\n",
    "\n",
    "    left_digits = np.digitize(x, bins)\n",
    "    if side == \"one\":\n",
    "        return left_digits\n",
    "\n",
    "    right_difits = np.digitize(x, bins, right=True)\n",
    "\n",
    "    rands = np.random.rand(len(x))  # uniform random numbers\n",
    "\n",
    "    digits = rands * (right_difits - left_digits) + left_digits\n",
    "    digits = np.ceil(digits).astype(np.int64)\n",
    "    return digits\n",
    "\n",
    "\n",
    "def binning(\n",
    "    row: Union[np.ndarray, torch.Tensor], n_bins: int\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    \"\"\"Binning the row into n_bins.\"\"\"\n",
    "    dtype = row.dtype\n",
    "    return_np = False if isinstance(row, torch.Tensor) else True\n",
    "    row = row.cpu().numpy() if isinstance(row, torch.Tensor) else row\n",
    "    # TODO: use torch.quantile and torch.bucketize\n",
    "\n",
    "    if row.max() == 0:\n",
    "        logger.warning(\n",
    "            \"The input data contains row of zeros. Please make sure this is expected.\"\n",
    "        )\n",
    "        return (\n",
    "            np.zeros_like(row, dtype=dtype)\n",
    "            if return_np\n",
    "            else torch.zeros_like(row, dtype=dtype)\n",
    "        )\n",
    "\n",
    "    if row.min() <= 0:\n",
    "        non_zero_ids = row.nonzero()\n",
    "        non_zero_row = row[non_zero_ids]\n",
    "        bins = np.quantile(non_zero_row, np.linspace(0, 1, n_bins - 1))\n",
    "        non_zero_digits = _digitize(non_zero_row, bins)\n",
    "        binned_row = np.zeros_like(row, dtype=np.int64)\n",
    "        binned_row[non_zero_ids] = non_zero_digits\n",
    "    else:\n",
    "        bins = np.quantile(row, np.linspace(0, 1, n_bins - 1))\n",
    "        binned_row = _digitize(row, bins)\n",
    "    return torch.from_numpy(binned_row) if not return_np else binned_row.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor_Edit(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=False,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "# Preprocess training and testing data\n",
    "preprocessor(adata, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "age_labels = adata.obs[\"age_id\"].tolist()  # make sure count from 0\n",
    "age_labels = np.array(age_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set number of samples: 1000, \n",
      "\t feature length: 2837\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = tokenize_and_pad_batch(\n",
    "    all_counts,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"data set number of samples: {tokenized_data['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_data['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        print(f\"{batch_labels_array=}\")\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def prepare_data(tokenized_data_dxe, sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values = random_mask_value(\n",
    "        tokenized_data_dxe[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    \n",
    "    input_gene_ids = tokenized_data_dxe[\"genes\"]\n",
    "    input_values = masked_values\n",
    "    target_values = tokenized_data_dxe[\"values\"]\n",
    "    tensor_age_labels = torch.from_numpy(age_labels).long()\n",
    "\n",
    "    data_pt = {\n",
    "        \"gene_ids\": input_gene_ids,\n",
    "        \"values\": input_values,\n",
    "        \"target_values\": target_values,\n",
    "    }\n",
    "\n",
    "    return data_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n",
      "Loading all model params from ./model.pt\n",
      "--------------------\n",
      "name: encoder.embedding.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.bias\n",
      "--------------------\n",
      "name: value_encoder.linear1.weight\n",
      "--------------------\n",
      "name: value_encoder.linear1.bias\n",
      "--------------------\n",
      "name: value_encoder.linear2.weight\n",
      "--------------------\n",
      "name: value_encoder.linear2.bias\n",
      "--------------------\n",
      "name: value_encoder.norm.weight\n",
      "--------------------\n",
      "name: value_encoder.norm.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.bias\n",
      "--------------------\n",
      "name: decoder.fc.0.weight\n",
      "--------------------\n",
      "name: decoder.fc.0.bias\n",
      "--------------------\n",
      "name: decoder.fc.2.weight\n",
      "--------------------\n",
      "name: decoder.fc.2.bias\n",
      "--------------------\n",
      "name: decoder.fc.4.weight\n",
      "--------------------\n",
      "name: decoder.fc.4.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.bias\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.weight\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.bias\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=None,\n",
    "    domain_spec_batchnorm=hyperparameter_defaults[\"DSBN\"],\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=hyperparameter_defaults[\"pre_norm\"],\n",
    ")\n",
    "if hyperparameter_defaults[\"load_model\"] is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "        print(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file, map_location=device)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            print(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    print(\"-\"*20)\n",
    "    print(f\"name: {name}\")\n",
    "    if hyperparameter_defaults[\"freeze\"] and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "if ADV:\n",
    "    discriminator = AdversarialDiscriminator(\n",
    "        d_model=embsize,\n",
    "        n_cls=num_batch_types,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, input_gene_ids, input_values):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        # Move data to device\n",
    "        input_gene_ids = torch.tensor(input_gene_ids).to(device)\n",
    "        input_values = torch.tensor(input_values).to(device)\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        \n",
    "#         with torch.cuda.amp.autocast(enabled=hyperparameter_defaults[\"amp\"]):\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=None,\n",
    "                CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=do_sample_in_train,\n",
    "                # generative_training = False,\n",
    "            )\n",
    "            output_values = output_dict[\"cls_output\"]\n",
    "            return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    all_counts = (\n",
    "        x.layers[input_layer_key].A\n",
    "        if issparse(x.layers[input_layer_key])\n",
    "        else x.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    age_labels = x.obs[\"age_id\"].tolist()  # make sure count from 0\n",
    "    age_labels = np.array(age_labels)\n",
    "\n",
    "    # Tokenize and pad a batch of data. Returns a list of tuple (gene_id, count)\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"age_labels\": torch.from_numpy(age_labels).long(),\n",
    "    }\n",
    "    \n",
    "    return test_data_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenized = tokenize(adata)\n",
    "gene_ids_tokenized = data_tokenized['gene_ids']\n",
    "values_tokenized = data_tokenized['values']\n",
    "\n",
    "num_samples = 1000\n",
    "sample_size = 100\n",
    "background_size = 100\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(num_samples, sample_size, replace=False)\n",
    "np.random.seed(24)\n",
    "background_indices = np.random.choice(num_samples, background_size, replace=False)\n",
    "\n",
    "sample_data = {\n",
    "    'gene_ids': gene_ids_tokenized[sample_indices],\n",
    "    'values': values_tokenized[sample_indices],\n",
    "}\n",
    "\n",
    "background_data = {\n",
    "    'gene_ids': gene_ids_tokenized[background_indices],\n",
    "    'values': values_tokenized[background_indices],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred  = evaluate(model, background_data['gene_ids'], background_data['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9980,  1.2646, -0.9831,  0.9999],\n",
       "        [-6.0103,  0.6579,  2.2665, -1.5088],\n",
       "        [ 8.1151, -0.0442, -2.2191, -2.8731],\n",
       "        [-5.3844, -0.7016,  1.4141,  0.0278],\n",
       "        [-4.0334, -1.4406,  4.1624, -1.4413],\n",
       "        [-3.7026, -1.5635,  0.5189,  1.2883],\n",
       "        [-2.7651, -2.2523, -0.8181,  4.7030],\n",
       "        [-5.9006, -0.8736,  2.7056, -0.4837],\n",
       "        [-1.9611, -2.0895, -1.6023,  5.1898],\n",
       "        [ 0.4272, -1.3041, -1.0851,  2.0024],\n",
       "        [-5.6489,  0.0294,  0.5312,  0.2782],\n",
       "        [-3.7749,  2.9418,  0.6464, -2.1891],\n",
       "        [ 8.0499, -0.0371, -2.2557, -2.7854],\n",
       "        [ 8.2493, -0.1994, -2.2436, -2.8290],\n",
       "        [-4.9783, -1.2129,  0.1304,  2.0930],\n",
       "        [-3.6091, -2.4759,  0.2529,  3.4584],\n",
       "        [-5.6139, -0.8703,  3.5868, -1.1431],\n",
       "        [ 8.1481, -0.0901, -2.2289, -2.8522],\n",
       "        [ 5.8468,  1.2667, -1.2594, -2.4659],\n",
       "        [ 8.2085, -0.2041, -2.2305, -2.7986],\n",
       "        [-2.5261, -1.4744, -1.6605,  4.9951],\n",
       "        [-5.9979, -0.3291,  3.2254, -1.3438],\n",
       "        [ 7.8174,  0.2092, -2.2527, -2.8159],\n",
       "        [-5.6188, -1.9841,  3.1997,  0.3848],\n",
       "        [ 1.0681,  3.6739, -1.0528, -2.3653],\n",
       "        [-3.9049, -3.2679,  2.3056,  1.8413],\n",
       "        [ 3.7801, -0.4090, -0.5609, -0.1261],\n",
       "        [-5.4303,  2.8473,  0.9461, -1.8254],\n",
       "        [-5.0300,  1.9368,  1.0486, -1.7643],\n",
       "        [-4.7766, -0.7137,  4.3673, -1.8892],\n",
       "        [ 8.0821, -0.0387, -2.2757, -2.7924],\n",
       "        [-5.6296,  0.9917,  0.9790, -0.8925],\n",
       "        [-3.5033, -1.2714,  0.6506,  0.6719],\n",
       "        [-2.2561, -0.7117, -1.4471,  2.8066],\n",
       "        [-2.8359, -2.4730,  0.8377,  1.8394],\n",
       "        [-2.8061,  1.0694, -0.9514,  0.1797],\n",
       "        [ 8.1182, -0.1288, -2.2073, -2.8107],\n",
       "        [-3.9679,  0.2603,  1.9195, -1.8891],\n",
       "        [-2.4494,  5.3389, -0.8245, -1.9096],\n",
       "        [-4.0241,  3.4207, -0.1260, -1.2600],\n",
       "        [ 7.5589,  0.3356, -2.1384, -2.8130],\n",
       "        [-4.2317, -3.2152,  3.2757,  1.2839],\n",
       "        [-3.0086, -1.3448, -1.0753,  3.7485],\n",
       "        [-1.7633, -2.5256, -1.2228,  5.0957],\n",
       "        [ 8.1857, -0.1974, -2.2846, -2.7276],\n",
       "        [-3.7964, -1.6049, -0.6001,  3.7373],\n",
       "        [-3.3157, -2.9270,  0.7352,  3.0951],\n",
       "        [-4.7590, -0.2321, -0.4691,  1.4198],\n",
       "        [ 8.0583, -0.0415, -2.2463, -2.7982],\n",
       "        [ 5.3584,  1.2497, -0.8240, -2.2768],\n",
       "        [ 8.1907, -0.1160, -2.2831, -2.8127],\n",
       "        [-2.6761, -2.7686, -0.3066,  4.6217],\n",
       "        [ 6.2149, -0.2686, -1.6488, -1.3095],\n",
       "        [-4.2431, -0.3992,  0.7924, -0.2154],\n",
       "        [ 8.1801, -0.0966, -2.2611, -2.8433],\n",
       "        [-5.5965, -1.0154,  3.4940, -0.9472],\n",
       "        [-3.6563, -2.9165,  0.8231,  3.1566],\n",
       "        [ 8.2351, -0.2070, -2.2535, -2.7972],\n",
       "        [-1.8598, -1.3886, -0.9167,  2.4893],\n",
       "        [-5.5802,  1.6684,  1.0904, -1.4379],\n",
       "        [-2.9652, -2.5427,  0.5948,  2.3186],\n",
       "        [ 6.4503,  1.0985, -1.4834, -2.8496],\n",
       "        [ 8.1379, -0.0261, -2.2512, -2.8783],\n",
       "        [-0.4611, -2.3709, -0.7684,  3.2424],\n",
       "        [-5.0849, -1.2563,  3.2751, -0.7979],\n",
       "        [ 6.0607,  0.2005, -1.3809, -1.7023],\n",
       "        [-5.4155,  0.1446,  2.6634, -1.7243],\n",
       "        [-4.3377,  3.4701,  0.7371, -2.1965],\n",
       "        [-3.2327, -0.3113,  0.5819, -0.4360],\n",
       "        [-3.7621,  3.6098, -0.3236, -1.2187],\n",
       "        [-4.8471,  2.9555,  0.6092, -1.7007],\n",
       "        [-1.0288, -0.2469,  0.4741, -0.9600],\n",
       "        [-2.8749, -2.9023, -0.1093,  4.6641],\n",
       "        [-4.3343, -1.3079, -0.1635,  2.3956],\n",
       "        [-6.1652,  1.0132,  1.3944, -1.0659],\n",
       "        [-2.4915, -2.2145, -0.6164,  3.9145],\n",
       "        [-4.9701, -0.7846,  4.2774, -1.7491],\n",
       "        [ 8.1991, -0.3923, -2.2041, -2.6405],\n",
       "        [-4.7390,  3.4378,  0.5718, -1.8050],\n",
       "        [-1.8037, -2.3524, -1.3759,  5.1158],\n",
       "        [ 7.9074,  0.0130, -2.2873, -2.6779],\n",
       "        [-1.9107, -0.6800,  2.2563, -1.9405],\n",
       "        [-3.3915, -1.5540,  0.4239,  1.2900],\n",
       "        [-5.6670, -1.4113,  2.4916,  0.1850],\n",
       "        [-4.9557,  0.2390, -0.7684,  1.3990],\n",
       "        [-4.4493, -1.9204,  0.6923,  1.8497],\n",
       "        [-2.2166,  4.8682, -0.3239, -2.3033],\n",
       "        [-5.9583, -0.3492,  2.1418, -0.6330],\n",
       "        [-2.8430, -2.3316, -0.8477,  5.0852],\n",
       "        [-6.1728, -0.0891,  2.0385, -0.6888],\n",
       "        [-5.8983, -0.6397,  3.7399, -1.3379],\n",
       "        [-6.0035,  1.0877, -0.2559,  0.6072],\n",
       "        [-5.8677, -1.3248,  3.1085, -0.2274],\n",
       "        [ 8.1624, -0.1018, -2.2636, -2.8194],\n",
       "        [-5.9875,  1.9682, -0.1960,  0.1541],\n",
       "        [-2.8242,  0.7937, -0.1998, -0.6275],\n",
       "        [-6.2283,  1.2058,  0.7207, -0.5211],\n",
       "        [-4.1326, -2.5515,  0.9524,  2.3417],\n",
       "        [-5.4524, -1.0828,  4.1764, -1.2190],\n",
       "        [-6.0648,  2.0675,  0.7009, -0.9692]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nUnable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# from tensorflow.python.layers import layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/saved_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SavedModelBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSavedModelBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/builder_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaver_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0m_np_float8_e4m3fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8_e4m3fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0m_np_float8_e5m2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8_e5m2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1127315/2681507720.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# wrap self.masker and self.model for output text explanation algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_transformers_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTeacherForcing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaskers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputComposite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shap/utils/transformers.py\u001b[0m in \u001b[0;36mis_transformers_lm\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\" Check if the given model object is a huggingface transformers language model.\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msafe_isinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.PreTrainedModel\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msafe_isinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.TFPreTrainedModel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0msafe_isinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODELS_FOR_SEQ_TO_SEQ_CAUSAL_LM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODELS_FOR_CAUSAL_LM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shap/utils/_general.py\u001b[0m in \u001b[0;36msafe_isinstance\u001b[0;34m(obj, class_path_str)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m#Get class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0m_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1538\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nUnable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "explainer = shap.Explainer(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2837])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import issparse\n",
    "x = adata.copy()\n",
    "all_counts = (\n",
    "        x.layers[input_layer_key].A\n",
    "        if issparse(x.layers[input_layer_key])\n",
    "        else x.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "age_labels = x.obs[\"age_id\"].tolist()  # make sure count from 0\n",
    "age_labels = np.array(age_labels)\n",
    "\n",
    "tokenized_test = tokenize_and_pad_batch(\n",
    "    all_counts,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "\n",
    "tokenized_test[\"genes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqxy699\u001b[0m (\u001b[33mqxy699-The University of Tennessee at Chattanooga\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/qxy699/scGPT/wandb/run-20240701_193128-9v8ew4bp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qxy699-The%20University%20of%20Tennessee%20at%20Chattanooga/scGPT/runs/9v8ew4bp' target=\"_blank\">astral-disco-2</a></strong> to <a href='https://wandb.ai/qxy699-The%20University%20of%20Tennessee%20at%20Chattanooga/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qxy699-The%20University%20of%20Tennessee%20at%20Chattanooga/scGPT' target=\"_blank\">https://wandb.ai/qxy699-The%20University%20of%20Tennessee%20at%20Chattanooga/scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qxy699-The%20University%20of%20Tennessee%20at%20Chattanooga/scGPT/runs/9v8ew4bp' target=\"_blank\">https://wandb.ai/qxy699-The%20University%20of%20Tennessee%20at%20Chattanooga/scGPT/runs/9v8ew4bp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'dataset_name': 'fly', 'do_train': False, 'load_model': './model.pt', 'mask_ratio': 0.0, 'epochs': 10, 'n_bins': 51, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 1, 'layer_size': 512, 'nlayers': 12, 'nhead': 8, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': False, 'freeze': False, 'DSBN': False}\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def f(tokenized_test = tokenized_test):\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        tv = tokenized_test[\"genes\"][0].cuda()\n",
    "        values_model = tokenized_test[\"values\"][0].cuda()\n",
    "        # print(f'{tv=}')\n",
    "        # attention_mask = (tv != 0).type(torch.int64).cuda()\n",
    "        src_key_padding_mask = tv.eq(vocab[pad_token]).cuda()\n",
    "        print(f\"{len(src_key_padding_mask)=}\")\n",
    "        outputs = model(tv, values_model, src_key_padding_mask=src_key_padding_mask)\n",
    "        scores = (np.exp(outputs[:2]).T / np.exp(outputs[:2]).sum(-1)).T\n",
    "        val = sp.special.logit(scores)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_dab = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            print(f\"{batch_data=}\")\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            # batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            # celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                print(f'{output_values.shape=}')\n",
    "                print(f'{target_values.shape=}')\n",
    "                loss = criterion_cls(output_values, target_values)\n",
    "\n",
    "                if DAB:\n",
    "                    print(f\"{output_dict['dab_output']=}\")\n",
    "                    loss_dab = criterion_dab(output_dict[\"dab_output\"], target_values)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids) if DAB else 0.0\n",
    "            total_num += len(input_gene_ids)\n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/err\": total_error / total_num,\n",
    "            \"valid/dab\": total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\": (total_loss + dab_weight * total_dab) / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_pt = prepare_data(tokenized_test, sort_seq_batch=per_seq_batch_sample)\n",
    "valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    ")\n",
    "# val_loss, val_err = evaluate(\n",
    "#         model,\n",
    "#         loader=valid_loader,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene_ids': tensor([[60695, 35664,  1302,  ..., 60694, 60694, 60694],\n",
       "         [60695, 35664,  1714,  ..., 60694, 60694, 60694],\n",
       "         [60695,  8034,  2852,  ..., 60694, 60694, 60694],\n",
       "         ...,\n",
       "         [60695, 31038, 11348,  ..., 60694, 60694, 60694],\n",
       "         [60695, 31038,  1302,  ..., 60694, 60694, 60694],\n",
       "         [60695,  1714,  1966,  ..., 60694, 60694, 60694]]),\n",
       " 'values': tensor([[ 0., 43., 42.,  ..., -2., -2., -2.],\n",
       "         [ 0., 24., 40.,  ..., -2., -2., -2.],\n",
       "         [ 0., 10., 13.,  ..., -2., -2., -2.],\n",
       "         ...,\n",
       "         [ 0.,  9., 27.,  ..., -2., -2., -2.],\n",
       "         [ 0., 25., 41.,  ..., -2., -2., -2.],\n",
       "         [ 0.,  7., 43.,  ..., -2., -2., -2.]]),\n",
       " 'target_values': tensor([[ 0., 43., 42.,  ..., -2., -2., -2.],\n",
       "         [ 0., 24., 40.,  ..., -2., -2., -2.],\n",
       "         [ 0., 10., 13.,  ..., -2., -2., -2.],\n",
       "         ...,\n",
       "         [ 0.,  9., 27.,  ..., -2., -2., -2.],\n",
       "         [ 0., 25., 41.,  ..., -2., -2., -2.],\n",
       "         [ 0.,  7., 43.,  ..., -2., -2., -2.]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'batch_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m input_values \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m target_values \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m celltype_labels \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcelltype_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'batch_labels'"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch_data in valid_loader:\n",
    "    input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "    input_values = batch_data[\"values\"].to(device)\n",
    "    target_values = batch_data[\"target_values\"].to(device)\n",
    "    batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "    celltype_labels = batch_data[\"celltype_labels\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene_ids': tensor([[60695, 35664,  1302,  ..., 60694, 60694, 60694]]),\n",
       " 'values': tensor([[ 0., 43., 42.,  ..., -2., -2., -2.]]),\n",
       " 'target_values': tensor([[ 0., 43., 42.,  ..., -2., -2., -2.]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(src_key_padding_mask)=2837\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m tv\u001b[38;5;241m.\u001b[39meq(vocab[pad_token])\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(src_key_padding_mask)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m scores \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mexp(outputs)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(outputs)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m      8\u001b[0m val \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mlogit(scores)\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/scgpt/model/model.py:372\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplicit_zero_prob:\n\u001b[1;32m    370\u001b[0m     output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlm_zero_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mlm_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 372\u001b[0m cell_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cell_emb_from_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_emb\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m cell_emb\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CLS:\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/scgpt/model/model.py:212\u001b[0m, in \u001b[0;36mTransformerModel._get_cell_emb_from_layer\u001b[0;34m(self, layer_output, weights)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    layer_output(:obj:`Tensor`): shape (batch, seq_len, embsize)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    :obj:`Tensor`: shape (batch, embsize)\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_emb_style \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 212\u001b[0m     cell_emb \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_emb_style \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg-pool\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    214\u001b[0m     cell_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(layer_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "    tv = tokenized_test[\"genes\"][0].cuda()\n",
    "    values_model = tokenized_test[\"values\"][0].cuda()\n",
    "    src_key_padding_mask = tv.eq(vocab[pad_token]).cuda()\n",
    "    print(f\"{len(src_key_padding_mask)=}\")\n",
    "    outputs = model(tv, values_model, src_key_padding_mask=src_key_padding_mask)\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2837])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"custom tokenizer\"\n",
    "\n",
    "# build an explainer by passing a transformers tokenizer\n",
    "if method == \"transformers tokenizer\":\n",
    "    explainer = shap.Explainer(f, tokenizer, output_names=labels)\n",
    "\n",
    "# build an explainer by explicitly creating a masker\n",
    "elif method == \"default masker\":\n",
    "    masker = shap.maskers.Text(r\"\\W\")  # this will create a basic whitespace tokenizer\n",
    "    explainer = shap.Explainer(f, masker, output_names=labels)\n",
    "\n",
    "# build a fully custom tokenizer\n",
    "elif method == \"custom tokenizer\":\n",
    "    import re\n",
    "\n",
    "    def custom_tokenizer(s, return_offsets_mapping=True):\n",
    "        \"\"\"Custom tokenizers conform to a subset of the transformers API.\"\"\"\n",
    "        pos = 0\n",
    "        offset_ranges = []\n",
    "        input_ids = []\n",
    "        for m in re.finditer(r\"\\W\", s):\n",
    "            start, end = m.span(0)\n",
    "            offset_ranges.append((pos, start))\n",
    "            input_ids.append(s[pos:start])\n",
    "            pos = end\n",
    "        if pos != len(s):\n",
    "            offset_ranges.append((pos, len(s)))\n",
    "            input_ids.append(s[pos:])\n",
    "        out = {}\n",
    "        out[\"input_ids\"] = input_ids\n",
    "        if return_offsets_mapping:\n",
    "            out[\"offset_mapping\"] = offset_ranges\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Text(custom_tokenizer)\n",
    "explainer = shap.Explainer(f, masker, output_names=age_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgene_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/shap/explainers/_partition.py:128\u001b[0m, in \u001b[0;36mPartitionExplainer.__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, fixed_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    126\u001b[0m              outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/shap/explainers/_explainer.py:266\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[0;32m--> 266\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    271\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/shap/explainers/_partition.py:151\u001b[0m, in \u001b[0;36mPartitionExplainer.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# if not fixed background or no base value assigned then compute base value for a row\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_base_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_background\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_base_value \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm00\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# the zero index param tells the masked model what the baseline is\u001b[39;00m\n\u001b[1;32m    152\u001b[0m f11 \u001b[38;5;241m=\u001b[39m fm(\u001b[38;5;241m~\u001b[39mm00\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker\u001b[38;5;241m.\u001b[39mclustering):\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/shap/utils/_masked_model.py:70\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_masking_call(full_masks, zero_index\u001b[38;5;241m=\u001b[39mzero_index, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/shap/utils/_masked_model.py:147\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    144\u001b[0m         all_masked_inputs[i]\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[1;32m    146\u001b[0m joined_masked_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([np\u001b[38;5;241m.\u001b[39mconcatenate(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_masked_inputs])\n\u001b[0;32m--> 147\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjoined_masked_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m _assert_output_input_match(joined_masked_inputs, outputs)\n\u001b[1;32m    149\u001b[0m all_outputs\u001b[38;5;241m.\u001b[39mappend(outputs)\n",
      "File \u001b[0;32m~/scGPTenv/lib/python3.10/site-packages/shap/models/_model.py:21\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     is_tensor \u001b[38;5;241m=\u001b[39m safe_isinstance(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m is_tensor \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(out)\n",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m, in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m----> 3\u001b[0m     tv \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenized_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      4\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m (tv \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(tv, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of dict"
     ]
    }
   ],
   "source": [
    "shap_values = explainer(adata.var[\"gene_name\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': ['DRG1'], 'offset_mapping': [(0, 4)]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(adata.var[\"gene_name\"][:5][0], return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(src_key_padding_mask)=2837\n",
      "Failed due to memory issue: CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 118.00 MiB is free. Process 34076 has 9.57 GiB memory in use. Including non-PyTorch memory, this process has 12.57 GiB memory in use. Process 1156862 has 1.41 GiB memory in use. Of the allocated memory 12.25 GiB is allocated by PyTorch, and 18.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = f(adata.var[\"gene_name\"][0])  # Assume you manage how this is passed to f\n",
    "    print(result)\n",
    "except RuntimeError as e:\n",
    "    print(\"Failed due to memory issue:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_key_padding_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43msrc_key_padding_mask\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'src_key_padding_mask' is not defined"
     ]
    }
   ],
   "source": [
    "len(src_key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4715,)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var[\"gene_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, ...,  1,  1, -1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_ids_in_vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 43,  0, ..., 36,  0,  0],\n",
       "       [ 0, 24,  0, ..., 45,  0,  0],\n",
       "       [10,  0,  0, ..., 32,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  9, ...,  0,  0,  0],\n",
       "       [ 0,  0, 25, ..., 18,  0,  0],\n",
       "       [ 0,  0,  0, ..., 43,  0,  0]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers[\"X_binned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8034, 35664, 31038, ..., 32191, 19734, 31375])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scGPTenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
