{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qxy699/hugging_face/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/qxy699/hugging_face/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "          2026,  2878,  2166,  1012]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-2.7276,  2.8789],\n",
      "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = torch.tensor([ids, ids])\n",
    "output = model(batched_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "          2026,  2878,  2166,  1012],\n",
       "        [ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "          2026,  2878,  2166,  1012]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i've been waiting for a huggingface course my whole life.\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "for id in batched_ids:\n",
    "    decoded_string = tokenizer.decode(id)\n",
    "    print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n",
      "Logits: tensor([[ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sequence1 = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "sequence2 = \"I hate this so much!\"\n",
    "\n",
    "tokens1 = tokenizer.tokenize(sequence1)\n",
    "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "\n",
    "tokens2 = tokenizer.tokenize(sequence2)\n",
    "ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
    "\n",
    "input_ids1 = torch.tensor([ids1])\n",
    "output1 = model(input_ids1)\n",
    "# print(\"Input IDs:\", input_ids)\n",
    "print(\"Logits:\", output1.logits)\n",
    "\n",
    "input_ids2 = torch.tensor([ids2])\n",
    "output2 = model(input_ids2)\n",
    "print(\"Logits:\", output2.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-2.7276,  2.8789],\n",
      "        [ 2.5423, -2.1265]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#padding them withoput using attention mask\n",
    "sequence1 = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "sequence2 = \"I hate this so much!\"\n",
    "\n",
    "tokens1 = tokenizer.tokenize(sequence1)\n",
    "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "\n",
    "tokens2 = tokenizer.tokenize(sequence2)\n",
    "ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
    "dst = torch.zeros_like(torch.tensor(ids1)).long()\n",
    "dst.put_(torch.tensor([range(len(ids2))]), torch.tensor(ids2))  #padding the ids with shortest length according to the longest ids with pytorch.YAY :)\n",
    "ids2 = dst\n",
    "\n",
    "batch_size = [ids1, ids2]\n",
    "output2 = model(torch.tensor(batch_size))\n",
    "print(\"Logits:\", output2.logits)\n",
    "\n",
    "# input_ids1 = torch.tensor([ids1])\n",
    "# input_ids2 = torch.tensor([ids2])\n",
    "\n",
    "# output2 = model(input_ids2)\n",
    "# print(\"Logits:\", output2.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result of two cells with addidng and without padding are not the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-2.7276,  2.8789],\n",
      "        [ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3491890/895642707.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask2_zero = torch.zeros_like(torch.tensor(ids2_pad))\n"
     ]
    }
   ],
   "source": [
    "#using attention mask with using attention mask\n",
    "sequence1 = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "sequence2 = \"I hate this so much!\"\n",
    "\n",
    "tokens1 = tokenizer.tokenize(sequence1)\n",
    "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "\n",
    "tokens2 = tokenizer.tokenize(sequence2)\n",
    "ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
    "dst = torch.zeros_like(torch.tensor(ids1)).long()\n",
    "dst.put_(torch.tensor([range(len(ids2))]), torch.tensor(ids2))  #padding the ids with shortest length according to the longest ids with pytorch.YAY :)\n",
    "ids2_pad = dst\n",
    "\n",
    "mask1 = torch.ones_like(torch.tensor(ids1))\n",
    "mask2 = torch.ones_like(torch.tensor(ids2))\n",
    "mask2_zero = torch.zeros_like(torch.tensor(ids2_pad))\n",
    "mask2_zero.put_(torch.tensor([range(len(ids2))]), mask2)\n",
    "\n",
    "attention_mask = torch.stack([mask1, mask2_zero])  #NEW\n",
    "batch_size = [ids1, ids2_pad]\n",
    "output2 = model(torch.tensor(batch_size), attention_mask=attention_mask)\n",
    "print(\"Logits:\", output2.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also the result is different when we use attenstion mask :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
