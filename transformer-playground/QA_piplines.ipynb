{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the question-answering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qxy699/hugging_face/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/qxy699/hugging_face/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9802603125572205,\n",
       "  'start': 78,\n",
       "  'end': 106,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow'},\n",
       " {'score': 0.008247792720794678,\n",
       "  'start': 78,\n",
       "  'end': 108,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow â€”'},\n",
       " {'score': 0.0013677021488547325,\n",
       "  'start': 78,\n",
       "  'end': 90,\n",
       "  'answer': 'Jax, PyTorch'},\n",
       " {'score': 0.00038108628359623253,\n",
       "  'start': 83,\n",
       "  'end': 106,\n",
       "  'answer': 'PyTorch, and TensorFlow'},\n",
       " {'score': 0.000216845452087, 'start': 96, 'end': 106, 'answer': 'TensorFlow'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "question_answerer(question=question, context=context, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letâ€™s see how it does all of this!\n",
    "1. start by tokenizing our input and \n",
    "2. then send it through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we tokenize the question and the context **as a pair** (https://huggingface.co/learn/nlp-course/chapter6/3b?fw=pt)\n",
    "\n",
    "the model has been trained **to predict the index of the token starting the answer (here 21) and the index of the token where the answer ends (here 24)**. This is why those models donâ€™t return one tensor of logits but **two**: one for the logits corresponding to the start token of the answer, and one for the logits corresponding to the end token of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-4.4952, -6.4454, -4.7115, -7.0968, -7.0726, -7.4981, -5.5397, -4.1368,\n",
       "         -5.9199, -5.4193, -1.5920, -1.0857, -5.0981, -2.9331, -3.4070,  2.2467,\n",
       "          5.1563, -1.3602, -2.2209, -0.9686, -4.8112, -2.2527,  1.4383, 10.1211,\n",
       "         -1.5311,  2.2685, -1.8951, -2.2108, -4.2142, -2.5571, -2.3252, -2.6046,\n",
       "          1.7047, -1.9867, -1.7211, -0.5415, -2.0239, -4.4246, -5.1012, -4.4966,\n",
       "         -7.8940, -6.7200, -4.6759, -6.3278, -4.8339, -5.1839, -3.3724, -7.4120,\n",
       "         -8.1542, -4.4871, -7.4659, -4.3293, -4.2293, -3.1903, -7.9467, -5.2665,\n",
       "         -7.5902, -5.0570, -7.4476, -7.9083, -6.5951, -7.4061, -8.8821, -7.6749,\n",
       "         -6.9879, -7.0466, -5.4193]], grad_fn=<CloneBackward0>), end_logits=tensor([[-2.3958e+00, -7.0978e+00, -7.0745e+00, -6.3676e+00, -5.9532e+00,\n",
       "         -7.9585e+00, -7.1869e+00, -3.6494e+00, -6.9677e+00, -5.1421e+00,\n",
       "         -3.1757e+00, -1.1649e+00, -7.0748e+00, -5.2875e+00, -6.8611e+00,\n",
       "         -5.1769e+00,  3.7892e+00, -4.4408e+00, -7.6688e-01, -3.9180e+00,\n",
       "         -2.1634e+00,  1.8116e+00, -1.4678e+00,  2.0508e+00,  1.5437e-03,\n",
       "         -1.5531e+00, -6.9469e-01, -1.3466e+00, -1.6879e+00,  4.0826e+00,\n",
       "          1.1467e+00, -3.7881e-01,  6.0774e-01,  1.2281e+00,  5.8202e-01,\n",
       "          1.0657e+01,  5.8794e+00, -5.7342e+00, -7.0719e+00, -6.8077e+00,\n",
       "         -7.1513e+00, -5.3228e+00, -3.4305e+00, -4.2575e+00,  2.2268e+00,\n",
       "         -4.1297e-01, -6.8944e+00, -7.9381e+00, -8.3298e+00, -5.6078e+00,\n",
       "         -8.9589e+00, -5.5772e+00, -5.7309e+00, -1.9592e+00, -7.8078e+00,\n",
       "         -2.3823e+00, -7.2457e+00, -6.1642e+00, -4.2830e+00, -8.0948e+00,\n",
       "         -8.0364e+00, -4.5566e+00, -7.6585e+00, -7.3241e+00, -2.2402e+00,\n",
       "         -1.8462e+00, -5.1420e+00]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAYEJAN ANGIZ! :D\n",
    "\n",
    "To convert those logits into probabilities, we will apply a softmax function â€” but before that, we need to make sure we mask the indices that are not part of the context:\n",
    "1. Masking Strategy: Before applying the softmax function to get probabilities from the modelâ€™s output logits, you need to mask certain tokens to focus the model on relevant parts of the input:\n",
    "Masking the Question and [SEP] Tokens: The idea is to ignore the parts of the input that don't directly contribute to answering the question, which includes the question itself and the separator tokens. This is because the answer is expected to be found within the context, not the question or the separators.\n",
    "Keeping the [CLS] Token: Although the [CLS] token is generally used for classification tasks, in some implementations, it may be used to indicate scenarios where the answer might not be explicitly present in the provided context.\n",
    "\n",
    "2. Implementing Masking by Using Large Negative Numbers: When computing the softmax of the logits (which are essentially the raw output scores from the model for each token), replacing certain logits with a large negative value (like -10000) effectively removes them from consideration. In the softmax function, such large negative values become zero in the final probability distribution, effectively excluding these tokens from influencing the answer.\n",
    "\n",
    "3. Softmax Application: After masking, applying the softmax function across the logits converts them into a probability distribution. Only the logits corresponding to the context tokens have meaningful values, and the probabilities associated with them will indicate the likelihood of each token being part of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " None,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " None]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.sequence_ids()  # it consider a number for each sentence query in order. for example question have id=0 and the context have id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5979,  1996,  3776,  9818,  1171,   100, 25267,   136,   102,\n",
       "           100, 25267,  1110,  5534,  1118,  1103,  1210,  1211,  1927,  1996,\n",
       "          3776,  9818,   783, 13612,   117,   153,  1183,  1942,  1766,  1732,\n",
       "           117,  1105,  5157, 21484,  2271,  6737,   783,  1114,   170,  2343,\n",
       "          1306,  2008,  9111,  1206,  1172,   119,  1135,   112,   188, 21546,\n",
       "          1106,  2669,  1240,  3584,  1114,  1141,  1196, 10745,  1172,  1111,\n",
       "          1107, 16792,  1114,  1103,  1168,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Which', 'deep', 'learning', 'libraries', 'back', '[UNK]', 'Transformers', '?', '[SEP]', '[UNK]', 'Transformers', 'is', 'backed', 'by', 'the', 'three', 'most', 'popular', 'deep', 'learning', 'libraries', 'â€”', 'Jax', ',', 'P', '##y', '##T', '##or', '##ch', ',', 'and', 'Ten', '##sor', '##F', '##low', 'â€”', 'with', 'a', 'sea', '##m', '##less', 'integration', 'between', 'them', '.', 'It', \"'\", 's', 'straightforward', 'to', 'train', 'your', 'models', 'with', 'one', 'before', 'loading', 'them', 'for', 'in', '##ference', 'with', 'the', 'other', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs.tokens())\n",
    "len(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.tokens()[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have properly masked the logits corresponding to positions we donâ€™t want to predict, we can apply the softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.4952e+00, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.5920e+00, -1.0857e+00, -5.0981e+00, -2.9331e+00, -3.4070e+00,\n",
       "          2.2467e+00,  5.1563e+00, -1.3602e+00, -2.2209e+00, -9.6861e-01,\n",
       "         -4.8112e+00, -2.2527e+00,  1.4383e+00,  1.0121e+01, -1.5311e+00,\n",
       "          2.2685e+00, -1.8951e+00, -2.2108e+00, -4.2142e+00, -2.5571e+00,\n",
       "         -2.3252e+00, -2.6046e+00,  1.7047e+00, -1.9867e+00, -1.7211e+00,\n",
       "         -5.4148e-01, -2.0239e+00, -4.4246e+00, -5.1012e+00, -4.4966e+00,\n",
       "         -7.8940e+00, -6.7200e+00, -4.6759e+00, -6.3278e+00, -4.8339e+00,\n",
       "         -5.1839e+00, -3.3724e+00, -7.4120e+00, -8.1542e+00, -4.4871e+00,\n",
       "         -7.4659e+00, -4.3293e+00, -4.2293e+00, -3.1903e+00, -7.9467e+00,\n",
       "         -5.2665e+00, -7.5902e+00, -5.0570e+00, -7.4476e+00, -7.9083e+00,\n",
       "         -6.5951e+00, -7.4061e+00, -8.8821e+00, -7.6749e+00, -6.9879e+00,\n",
       "         -7.0466e+00, -1.0000e+04]], grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3958e+00, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -3.1757e+00, -1.1649e+00, -7.0748e+00, -5.2875e+00, -6.8611e+00,\n",
       "         -5.1769e+00,  3.7892e+00, -4.4408e+00, -7.6688e-01, -3.9180e+00,\n",
       "         -2.1634e+00,  1.8116e+00, -1.4678e+00,  2.0508e+00,  1.5437e-03,\n",
       "         -1.5531e+00, -6.9469e-01, -1.3466e+00, -1.6879e+00,  4.0826e+00,\n",
       "          1.1467e+00, -3.7881e-01,  6.0774e-01,  1.2281e+00,  5.8202e-01,\n",
       "          1.0657e+01,  5.8794e+00, -5.7342e+00, -7.0719e+00, -6.8077e+00,\n",
       "         -7.1513e+00, -5.3228e+00, -3.4305e+00, -4.2575e+00,  2.2268e+00,\n",
       "         -4.1297e-01, -6.8944e+00, -7.9381e+00, -8.3298e+00, -5.6078e+00,\n",
       "         -8.9589e+00, -5.5772e+00, -5.7309e+00, -1.9592e+00, -7.8078e+00,\n",
       "         -2.3823e+00, -7.2457e+00, -6.1642e+00, -4.2830e+00, -8.0948e+00,\n",
       "         -8.0364e+00, -4.5566e+00, -7.6585e+00, -7.3241e+00, -2.2402e+00,\n",
       "         -1.8462e+00, -1.0000e+04]], grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.4531e-07, 6.3342e-08, 3.5868e-07, 3.3020e-08, 3.3829e-08, 2.2105e-08,\n",
       "        1.5668e-07, 6.3723e-07, 1.0713e-07, 1.7673e-07, 8.1185e-06, 1.3470e-05,\n",
       "        2.4368e-07, 2.1236e-06, 1.3220e-06, 3.7722e-04, 6.9219e-03, 1.0237e-05,\n",
       "        4.3289e-06, 1.5143e-05, 3.2463e-07, 4.1933e-06, 1.6808e-04, 9.9179e-01,\n",
       "        8.6288e-06, 3.8557e-04, 5.9956e-06, 4.3725e-06, 5.8977e-07, 3.0929e-06,\n",
       "        3.8998e-06, 2.9493e-06, 2.1940e-04, 5.4713e-06, 7.1354e-06, 2.3212e-05,\n",
       "        5.2711e-06, 4.7788e-07, 2.4291e-07, 4.4467e-07, 1.4879e-08, 4.8133e-08,\n",
       "        3.7169e-07, 7.1242e-08, 3.1735e-07, 2.2365e-07, 1.3685e-06, 2.4093e-08,\n",
       "        1.1470e-08, 4.4891e-07, 2.2828e-08, 5.2562e-07, 5.8092e-07, 1.6419e-06,\n",
       "        1.4114e-08, 2.0591e-07, 2.0161e-08, 2.5390e-07, 2.3251e-08, 1.4667e-08,\n",
       "        5.4532e-08, 2.4235e-08, 5.5390e-09, 1.8524e-08, 3.6818e-08, 3.4721e-08,\n",
       "        1.7673e-07], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]\n",
    "start_probabilities  #torch.Size([67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1185e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7129e-07, 7.2546e-06,\n",
       "        1.9679e-08, 1.1754e-07, 2.4366e-08, 1.3130e-07, 1.0284e-03, 2.7411e-07,\n",
       "        1.0801e-05, 4.6232e-07, 2.6730e-06, 1.4233e-04, 5.3586e-06, 1.8078e-04,\n",
       "        2.3292e-05, 4.9208e-06, 1.1610e-05, 6.0494e-06, 4.3002e-06, 1.3790e-03,\n",
       "        7.3201e-05, 1.5923e-05, 4.2704e-05, 7.9413e-05, 4.1620e-05, 9.8838e-01,\n",
       "        8.3161e-03, 7.5197e-08, 1.9735e-08, 2.5704e-08, 1.8229e-08, 1.1346e-07,\n",
       "        7.5278e-07, 3.2926e-07, 2.1558e-04, 1.5388e-05, 2.3568e-08, 8.2993e-09,\n",
       "        5.6096e-09, 8.5332e-08, 2.9904e-09, 8.7980e-08, 7.5446e-08, 3.2784e-06,\n",
       "        9.4549e-09, 2.1473e-06, 1.6586e-08, 4.8915e-08, 3.2096e-07, 7.0959e-09,\n",
       "        7.5224e-09, 2.4413e-07, 1.0978e-08, 1.5336e-08, 2.4753e-06, 3.6704e-06,\n",
       "        0.0000e+00], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_probabilities  #torch.Size([67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([67, 1])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_probabilities[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_probabilities[None, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could take the argmax of the start and end probabilities â€” but **we might end up with a start index that is greater than the end index**, so we need to take a few more precautions. We will compute the probabilities of each possible start_index and end_index where start_index <= end_index, then take the tuple (start_index, end_index) with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  10,  100, 1000],\n",
       "        [  20,  200, 2000],\n",
       "        [  30,  300, 3000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "test = torch.tensor([[1], [2], [3]]) * torch.tensor([10, 100,1000])\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JADID! :D (torch.triu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  10,  100, 1000],\n",
       "        [   0,  200, 2000],\n",
       "        [   0,    0, 3000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_test = torch.triu(test)\n",
    "score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index_test = score_test.argmax().item()\n",
    "max_index_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POINT: to find out the index of a value from a flatten matrix in its original dimensional matrix we should divide the index from flatten matrix to the number of columns of the original matrix. the answer is the row n umber and the module is the coumn number of the value in the original matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index_test = max_index_test // score_test.shape[1]\n",
    "start_index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_index_test = max_index_test % score_test.shape[1]\n",
    "end_index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KHAFAN!: learn how it add dimension to the tensors :D\n",
    "scores = start_probabilities[:, None] * end_probabilities[None, :] #size(67, 67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then weâ€™ll mask the values where start_index > end_index by setting them to 0 (the other probabilities are all positive numbers). The torch.triu() function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.triu(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9803, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-------------------------EXAM------------------------\n",
    "\n",
    "Try it out! Compute the start and end indices for the five most likely answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1576, 1577, 1107, 1570, 1557])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattens = torch.flatten(scores)\n",
    "sorted, indices = torch.sort(flattens, descending=True)\n",
    "indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9803, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0082, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0068, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0014, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0010, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for max_index_ex in indices[:5]:\n",
    "    start_index_ex = max_index_ex // scores.shape[1]\n",
    "    end_index_ex = max_index_ex % scores.shape[1]\n",
    "    print(scores[start_index_ex, end_index_ex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------------------END------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Jax, PyTorch, and TensorFlow',\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'score': 0.9802576899528503}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]\n",
    "\n",
    "results = {\"answer\": answer,\n",
    "           \"start\": start_char,\n",
    "           \"end\": end_char,\n",
    "           \"score\": scores[start_index, end_index].item()}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------------EXAM----------------------------------\n",
    "\n",
    "Try it out! Use the best scores you computed earlier to show the five most likely answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'Jax, PyTorch, and TensorFlow',\n",
       "  'start': 78,\n",
       "  'end': 106,\n",
       "  'score': tensor(0.9803, grad_fn=<SelectBackward0>)},\n",
       " {'answer': 'Jax, PyTorch, and TensorFlow â€”',\n",
       "  'start': 78,\n",
       "  'end': 108,\n",
       "  'score': tensor(0.0082, grad_fn=<SelectBackward0>)},\n",
       " {'answer': 'three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow',\n",
       "  'start': 33,\n",
       "  'end': 106,\n",
       "  'score': tensor(0.0068, grad_fn=<SelectBackward0>)},\n",
       " {'answer': 'Jax, PyTorch',\n",
       "  'start': 78,\n",
       "  'end': 90,\n",
       "  'score': tensor(0.0014, grad_fn=<SelectBackward0>)},\n",
       " {'answer': '',\n",
       "  'start': 78,\n",
       "  'end': 38,\n",
       "  'score': tensor(0.0010, grad_fn=<SelectBackward0>)}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ex = []\n",
    "for max_index_ex in indices[:5]:\n",
    "    start_index_ex = max_index_ex // scores.shape[1]\n",
    "    end_index_ex = max_index_ex % scores.shape[1]\n",
    "\n",
    "    start_char_ex, _ = offsets[start_index_ex]\n",
    "    _, end_char_ex = offsets[end_index_ex]\n",
    "\n",
    "    answer_ex = context[start_char_ex: end_char_ex]\n",
    "\n",
    "    score_ex = scores[start_index_ex, end_index_ex]\n",
    "\n",
    "    results_ex.append({'answer': answer_ex,\n",
    "                       \"start\": start_char_ex,\n",
    "                       \"end\": end_char_ex,\n",
    "                       \"score\": score_ex})\n",
    "\n",
    "results_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same!!!!!!!!!!!!!!!!! =D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling long contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9714871048927307,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "ðŸ¤— Transformers: State of the Art NLP\n",
    "\n",
    "ðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, long_context)\n",
    "print(len(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 5979, 1996, 3776, 9818, 1171, 100, 25267, 136, 102, 100, 25267, 131, 1426, 1104, 1103, 2051, 21239, 2101, 100, 25267, 2790, 4674, 1104, 3073, 4487, 9044, 3584, 1106, 3870, 8249, 1113, 6685, 1216, 1112, 5393, 117, 1869, 16026, 117, 2304, 10937, 117, 7584, 7317, 2734, 117, 5179, 117, 3087, 3964, 1105, 1167, 1107, 1166, 1620, 3483, 119, 2098, 6457, 1110, 1106, 1294, 5910, 118, 2652, 21239, 2101, 5477, 1106, 1329, 1111, 2490, 119, 100, 25267, 2790, 20480, 1116, 1106, 1976, 9133, 1105, 1329, 1343, 3073, 4487, 9044, 3584, 1113, 170, 1549, 3087, 117, 2503, 118, 9253, 1172, 1113, 1240, 1319, 2233, 27948, 1105, 1173, 2934, 1172, 1114, 1103, 1661, 1113, 1412, 2235, 10960, 119, 1335, 1103, 1269, 1159, 117, 1296, 185, 25669, 8613, 13196, 13682, 1126, 4220, 1110, 3106, 2484, 20717, 1673, 1105, 1169, 1129, 5847, 1106, 9396, 3613, 1844, 7857, 119, 2009, 1431, 146, 1329, 11303, 1468, 136, 122, 119, 12167, 118, 1106, 118, 1329, 1352, 118, 1104, 118, 1103, 118, 1893, 3584, 131, 118, 1693, 2099, 1113, 21239, 2591, 1105, 21239, 2349, 8249, 119, 118, 8274, 9391, 1106, 3990, 1111, 24937, 1105, 16681, 119, 118, 17751, 4795, 118, 4749, 11108, 5266, 1114, 1198, 1210, 3553, 1106, 3858, 119, 118, 138, 13943, 20480, 1111, 1606, 1155, 1412, 3073, 4487, 9044, 3584, 119, 118, 5738, 3254, 22662, 4692, 117, 2964, 6302, 2555, 10988, 131, 123, 119, 26982, 1169, 2934, 3972, 3584, 1939, 1104, 1579, 1231, 4487, 16534, 119, 118, 153, 19366, 3121, 2116, 1468, 1169, 4851, 3254, 22662, 1159, 1105, 1707, 4692, 119, 118, 2091, 10947, 1116, 1104, 4220, 1116, 1114, 1166, 1275, 117, 1288, 3073, 4487, 9044, 3584, 117, 1199, 1107, 1167, 1190, 1620, 3483, 119, 124, 119, 22964, 6787, 1103, 1268, 8297, 1111, 1451, 1226, 1104, 170, 2235, 112, 188, 7218, 131, 118, 9791, 1352, 118, 1104, 118, 1103, 118, 1893, 3584, 1107, 124, 2442, 1104, 3463, 119, 118, 15729, 170, 1423, 2235, 1206, 157, 2271, 1477, 119, 121, 120, 153, 1183, 1942, 1766, 1732, 8297, 1116, 1120, 1209, 119, 118, 3017, 1306, 8709, 3368, 1103, 1268, 8297, 1111, 2013, 117, 10540, 1105, 1707, 119, 125, 119, 142, 20158, 8156, 3708, 170, 2235, 1137, 1126, 1859, 1106, 1240, 2993, 131, 118, 1284, 2194, 5136, 1111, 1296, 4220, 1106, 23577, 1103, 2686, 1502, 1118, 1157, 1560, 5752, 119, 118, 6747, 4422, 102], [101, 5979, 1996, 3776, 9818, 1171, 100, 25267, 136, 102, 2091, 10947, 1116, 1104, 4220, 1116, 1114, 1166, 1275, 117, 1288, 3073, 4487, 9044, 3584, 117, 1199, 1107, 1167, 1190, 1620, 3483, 119, 124, 119, 22964, 6787, 1103, 1268, 8297, 1111, 1451, 1226, 1104, 170, 2235, 112, 188, 7218, 131, 118, 9791, 1352, 118, 1104, 118, 1103, 118, 1893, 3584, 1107, 124, 2442, 1104, 3463, 119, 118, 15729, 170, 1423, 2235, 1206, 157, 2271, 1477, 119, 121, 120, 153, 1183, 1942, 1766, 1732, 8297, 1116, 1120, 1209, 119, 118, 3017, 1306, 8709, 3368, 1103, 1268, 8297, 1111, 2013, 117, 10540, 1105, 1707, 119, 125, 119, 142, 20158, 8156, 3708, 170, 2235, 1137, 1126, 1859, 1106, 1240, 2993, 131, 118, 1284, 2194, 5136, 1111, 1296, 4220, 1106, 23577, 1103, 2686, 1502, 1118, 1157, 1560, 5752, 119, 118, 6747, 4422, 1116, 1132, 5490, 1112, 10887, 1112, 1936, 119, 118, 6747, 7004, 1169, 1129, 1215, 8942, 1104, 1103, 3340, 1111, 3613, 7857, 119, 100, 25267, 1110, 5534, 1118, 1103, 1210, 1211, 1927, 1996, 3776, 9818, 783, 13612, 117, 153, 1183, 1942, 1766, 1732, 1105, 5157, 21484, 2271, 6737, 783, 1114, 170, 2343, 1306, 2008, 9111, 1206, 1172, 119, 1135, 112, 188, 21546, 1106, 2669, 1240, 3584, 1114, 1141, 1196, 10745, 1172, 1111, 1107, 16792, 1114, 1103, 1168, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 5), (6, 10), (11, 19), (20, 29), (30, 34), (35, 36), (37, 49), (49, 50), (0, 0), (1, 2), (3, 15), (15, 16), (17, 22), (23, 25), (26, 29), (30, 33), (34, 36), (36, 37), (39, 40), (41, 53), (54, 62), (63, 72), (73, 75), (76, 79), (79, 82), (82, 86), (87, 93), (94, 96), (97, 104), (105, 110), (111, 113), (114, 119), (120, 124), (125, 127), (128, 142), (142, 143), (144, 155), (156, 166), (166, 167), (168, 176), (177, 186), (186, 187), (188, 191), (191, 194), (194, 201), (201, 202), (203, 214), (214, 215), (216, 220), (221, 231), (232, 235), (236, 240), (241, 243), (244, 248), (249, 252), (253, 262), (262, 263), (264, 267), (268, 271), (272, 274), (275, 277), (278, 282), (283, 290), (290, 291), (291, 295), (296, 298), (298, 299), (300, 306), (307, 309), (310, 313), (314, 317), (318, 326), (326, 327), (329, 330), (331, 343), (344, 352), (353, 356), (356, 357), (358, 360), (361, 368), (369, 377), (378, 381), (382, 385), (386, 391), (392, 395), (395, 398), (398, 402), (403, 409), (410, 412), (413, 414), (415, 420), (421, 425), (425, 426), (427, 431), (431, 432), (432, 436), (437, 441), (442, 444), (445, 449), (450, 453), (454, 458), (458, 462), (463, 466), (467, 471), (472, 477), (478, 482), (483, 487), (488, 491), (492, 501), (502, 504), (505, 508), (509, 514), (515, 518), (518, 519), (520, 522), (523, 526), (527, 531), (532, 536), (536, 537), (538, 542), (543, 544), (544, 546), (546, 549), (550, 556), (557, 565), (566, 568), (569, 581), (582, 584), (585, 590), (591, 596), (596, 599), (599, 601), (602, 605), (606, 609), (610, 612), (613, 621), (622, 624), (625, 631), (632, 637), (638, 646), (647, 658), (658, 659), (661, 664), (665, 671), (672, 673), (674, 677), (678, 687), (687, 690), (690, 691), (693, 694), (694, 695), (696, 700), (700, 701), (701, 703), (703, 704), (704, 707), (708, 713), (713, 714), (714, 716), (716, 717), (717, 720), (720, 721), (721, 724), (725, 731), (731, 732), (735, 736), (737, 741), (742, 753), (754, 756), (757, 759), (759, 760), (761, 764), (765, 767), (767, 768), (769, 774), (774, 775), (778, 779), (780, 783), (784, 791), (792, 794), (795, 800), (801, 804), (805, 814), (815, 818), (819, 832), (832, 833), (836, 837), (838, 841), (842, 846), (846, 847), (847, 853), (854, 862), (862, 866), (867, 871), (872, 876), (877, 882), (883, 890), (891, 893), (894, 899), (899, 900), (903, 904), (905, 906), (907, 914), (915, 918), (919, 922), (923, 928), (929, 932), (933, 936), (937, 940), (940, 943), (943, 947), (948, 954), (954, 955), (958, 959), (960, 965), (966, 969), (969, 973), (974, 979), (979, 980), (981, 988), (989, 995), (996, 1000), (1000, 1005), (1005, 1006), (1008, 1009), (1009, 1010), (1011, 1022), (1023, 1026), (1027, 1032), (1033, 1040), (1041, 1047), (1048, 1055), (1056, 1058), (1059, 1065), (1066, 1068), (1068, 1071), (1071, 1076), (1076, 1077), (1080, 1081), (1082, 1083), (1083, 1086), (1086, 1088), (1088, 1092), (1092, 1095), (1096, 1099), (1100, 1106), (1107, 1110), (1110, 1114), (1115, 1119), (1120, 1123), (1124, 1134), (1135, 1140), (1140, 1141), (1144, 1145), (1146, 1148), (1148, 1151), (1151, 1152), (1153, 1155), (1156, 1168), (1168, 1169), (1170, 1174), (1175, 1179), (1180, 1182), (1182, 1183), (1183, 1186), (1187, 1190), (1190, 1193), (1193, 1197), (1198, 1204), (1204, 1205), (1206, 1210), (1211, 1213), (1214, 1218), (1219, 1223), (1224, 1227), (1228, 1237), (1237, 1238), (1240, 1241), (1241, 1242), (1243, 1246), (1246, 1249), (1250, 1253), (1254, 1259), (1260, 1269), (1270, 1273), (1274, 1279), (1280, 1284), (1285, 1287), (1288, 1289), (1290, 1295), (1295, 1296), (1296, 1297), (1298, 1306), (1306, 1307), (1310, 1311), (1312, 1317), (1318, 1323), (1323, 1324), (1324, 1326), (1326, 1327), (1327, 1330), (1330, 1331), (1331, 1334), (1335, 1341), (1342, 1344), (1345, 1346), (1347, 1352), (1353, 1355), (1356, 1360), (1360, 1361), (1364, 1365), (1366, 1370), (1371, 1372), (1373, 1379), (1380, 1385), (1386, 1393), (1394, 1395), (1395, 1396), (1396, 1397), (1397, 1398), (1398, 1399), (1399, 1400), (1400, 1401), (1401, 1402), (1402, 1403), (1403, 1405), (1405, 1407), (1408, 1417), (1417, 1418), (1419, 1421), (1422, 1426), (1426, 1427), (1430, 1431), (1432, 1435), (1435, 1436), (1436, 1442), (1443, 1447), (1448, 1451), (1452, 1457), (1458, 1467), (1468, 1471), (1472, 1480), (1480, 1481), (1482, 1492), (1493, 1496), (1497, 1507), (1507, 1508), (1510, 1511), (1511, 1512), (1513, 1514), (1514, 1519), (1520, 1526), (1526, 1529), (1530, 1531), (1532, 1537), (1538, 1540), (1541, 1543), (1544, 1551), (1552, 1554), (1555, 1559), (1560, 1565), (1565, 1566), (1569, 1570), (1571, 1573), (1574, 1581), (1582, 1590), (1591, 1594), (1595, 1599), (1600, 1612), (1613, 1615), (1616, 1625), (1626, 1629), (1630, 1637), (1638, 1647), (1648, 1650), (1651, 1654), (1655, 1663), (1664, 1671), (1671, 1672), (1675, 1676), (1677, 1682), (1683, 1691), (0, 0)], [(0, 0), (0, 5), (6, 10), (11, 19), (20, 29), (30, 34), (35, 36), (37, 49), (49, 50), (0, 0), (1146, 1148), (1148, 1151), (1151, 1152), (1153, 1155), (1156, 1168), (1168, 1169), (1170, 1174), (1175, 1179), (1180, 1182), (1182, 1183), (1183, 1186), (1187, 1190), (1190, 1193), (1193, 1197), (1198, 1204), (1204, 1205), (1206, 1210), (1211, 1213), (1214, 1218), (1219, 1223), (1224, 1227), (1228, 1237), (1237, 1238), (1240, 1241), (1241, 1242), (1243, 1246), (1246, 1249), (1250, 1253), (1254, 1259), (1260, 1269), (1270, 1273), (1274, 1279), (1280, 1284), (1285, 1287), (1288, 1289), (1290, 1295), (1295, 1296), (1296, 1297), (1298, 1306), (1306, 1307), (1310, 1311), (1312, 1317), (1318, 1323), (1323, 1324), (1324, 1326), (1326, 1327), (1327, 1330), (1330, 1331), (1331, 1334), (1335, 1341), (1342, 1344), (1345, 1346), (1347, 1352), (1353, 1355), (1356, 1360), (1360, 1361), (1364, 1365), (1366, 1370), (1371, 1372), (1373, 1379), (1380, 1385), (1386, 1393), (1394, 1395), (1395, 1396), (1396, 1397), (1397, 1398), (1398, 1399), (1399, 1400), (1400, 1401), (1401, 1402), (1402, 1403), (1403, 1405), (1405, 1407), (1408, 1417), (1417, 1418), (1419, 1421), (1422, 1426), (1426, 1427), (1430, 1431), (1432, 1435), (1435, 1436), (1436, 1442), (1443, 1447), (1448, 1451), (1452, 1457), (1458, 1467), (1468, 1471), (1472, 1480), (1480, 1481), (1482, 1492), (1493, 1496), (1497, 1507), (1507, 1508), (1510, 1511), (1511, 1512), (1513, 1514), (1514, 1519), (1520, 1526), (1526, 1529), (1530, 1531), (1532, 1537), (1538, 1540), (1541, 1543), (1544, 1551), (1552, 1554), (1555, 1559), (1560, 1565), (1565, 1566), (1569, 1570), (1571, 1573), (1574, 1581), (1582, 1590), (1591, 1594), (1595, 1599), (1600, 1612), (1613, 1615), (1616, 1625), (1626, 1629), (1630, 1637), (1638, 1647), (1648, 1650), (1651, 1654), (1655, 1663), (1664, 1671), (1671, 1672), (1675, 1676), (1677, 1682), (1683, 1691), (1691, 1692), (1693, 1696), (1697, 1704), (1705, 1707), (1708, 1720), (1721, 1723), (1724, 1732), (1732, 1733), (1736, 1737), (1738, 1743), (1744, 1749), (1750, 1753), (1754, 1756), (1757, 1761), (1762, 1775), (1776, 1778), (1779, 1782), (1783, 1790), (1791, 1794), (1795, 1800), (1801, 1812), (1812, 1813), (1815, 1816), (1817, 1829), (1830, 1832), (1833, 1839), (1840, 1842), (1843, 1846), (1847, 1852), (1853, 1857), (1858, 1865), (1866, 1870), (1871, 1879), (1880, 1889), (1890, 1891), (1892, 1895), (1895, 1896), (1897, 1898), (1898, 1899), (1899, 1900), (1900, 1902), (1902, 1904), (1905, 1908), (1909, 1912), (1912, 1915), (1915, 1916), (1916, 1919), (1920, 1921), (1922, 1926), (1927, 1928), (1929, 1932), (1932, 1933), (1933, 1937), (1938, 1949), (1950, 1957), (1958, 1962), (1962, 1963), (1964, 1966), (1966, 1967), (1967, 1968), (1969, 1984), (1985, 1987), (1988, 1993), (1994, 1998), (1999, 2005), (2006, 2010), (2011, 2014), (2015, 2021), (2022, 2029), (2030, 2034), (2035, 2038), (2039, 2041), (2041, 2048), (2049, 2053), (2054, 2057), (2058, 2063), (2063, 2064), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "#Since overflow_to_sample_mapping and offset_mapping are not parameters used by \n",
    "# the model, weâ€™ll pop them out of the inputs, before converting it to a tensor\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.attention_mask[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Like before, we first mask the tokens that are not part of the context before \n",
    "# taking the softmax. We also mask all the padding tokens (as flagged by the \n",
    "# attention mask):\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "# Mask all the [PAD] tokens\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 18, 0.33867067098617554), (173, 184, 0.9714868664741516)]\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '\\nðŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867067098617554}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714868664741516}\n"
     ]
    }
   ],
   "source": [
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
