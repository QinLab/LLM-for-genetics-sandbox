{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPiece is the tokenization algorithm Google developed to **pretrain BERT**. It has since been reused in quite a few Transformer models **based on BERT**, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET.\n",
    "\n",
    "WordPiece:\n",
    "\n",
    "1. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet.\n",
    "2. Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, \"word\" gets split like this: \"w ##o ##r ##d\".Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n",
    "3. Then, again like BPE, WordPiece learns merge rules. Instead of selecting the most frequent pair, WordPiece computes a score for each pair (score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)) IDEA KHAFAN: By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. For instance, it won’t necessarily merge (\"un\", \"##able\") even if that pair occurs very frequently in the vocabulary, because the two pairs \"un\" and \"##able\" will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like (\"hu\", \"##gging\") will probably be merged faster (assuming the word “hugging” appears often in the vocabulary) since \"hu\" and \"##gging\" are likely to be less frequent individually. (https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)\n",
    "4. Tokenization differs in WordPiece and BPE in that WordPiece only **saves the final vocabulary, not the merge rules learned.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to pre-tokenize the corpus into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qxy699/hugging_face/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 3,\n",
       " 'is': 2,\n",
       " 'the': 1,\n",
       " 'Hugging': 1,\n",
       " 'Face': 1,\n",
       " 'Course': 1,\n",
       " '.': 4,\n",
       " 'chapter': 1,\n",
       " 'about': 1,\n",
       " 'tokenization': 1,\n",
       " 'section': 1,\n",
       " 'shows': 1,\n",
       " 'several': 1,\n",
       " 'tokenizer': 1,\n",
       " 'algorithms': 1,\n",
       " 'Hopefully': 1,\n",
       " ',': 1,\n",
       " 'you': 1,\n",
       " 'will': 1,\n",
       " 'be': 1,\n",
       " 'able': 1,\n",
       " 'to': 1,\n",
       " 'understand': 1,\n",
       " 'how': 1,\n",
       " 'they': 1,\n",
       " 'are': 1,\n",
       " 'trained': 1,\n",
       " 'and': 1,\n",
       " 'generate': 1,\n",
       " 'tokens': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs = dict()\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    words = [words for words, offset in words_with_offsets]\n",
    "    for word in words:\n",
    "        if word not in word_freqs.keys():\n",
    "            word_freqs[word] = 1\n",
    "        else:\n",
    "            word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the alphabet: the alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##a',\n",
       " '##b',\n",
       " '##c',\n",
       " '##d',\n",
       " '##e',\n",
       " '##f',\n",
       " '##g',\n",
       " '##h',\n",
       " '##i',\n",
       " '##k',\n",
       " '##l',\n",
       " '##m',\n",
       " '##n',\n",
       " '##o',\n",
       " '##p',\n",
       " '##r',\n",
       " '##s',\n",
       " '##t',\n",
       " '##u',\n",
       " '##v',\n",
       " '##w',\n",
       " '##y',\n",
       " '##z',\n",
       " ',',\n",
       " '.',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'w',\n",
       " 'y']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for words in word_freqs.keys():\n",
    "    if words[0] not in alphabet:\n",
    "        alphabet.append(words[0])\n",
    "    for chr in words[1:]:\n",
    "        letter = \"##\" + chr\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it’s the list [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', '##h', '##i', '##s'],\n",
       " 'is': ['i', '##s'],\n",
       " 'the': ['t', '##h', '##e'],\n",
       " 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n",
       " 'Face': ['F', '##a', '##c', '##e'],\n",
       " 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n",
       " '.': ['.'],\n",
       " 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n",
       " 'about': ['a', '##b', '##o', '##u', '##t'],\n",
       " 'tokenization': ['t',\n",
       "  '##o',\n",
       "  '##k',\n",
       "  '##e',\n",
       "  '##n',\n",
       "  '##i',\n",
       "  '##z',\n",
       "  '##a',\n",
       "  '##t',\n",
       "  '##i',\n",
       "  '##o',\n",
       "  '##n'],\n",
       " 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n",
       " 'shows': ['s', '##h', '##o', '##w', '##s'],\n",
       " 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n",
       " 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],\n",
       " 'algorithms': ['a',\n",
       "  '##l',\n",
       "  '##g',\n",
       "  '##o',\n",
       "  '##r',\n",
       "  '##i',\n",
       "  '##t',\n",
       "  '##h',\n",
       "  '##m',\n",
       "  '##s'],\n",
       " 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n",
       " ',': [','],\n",
       " 'you': ['y', '##o', '##u'],\n",
       " 'will': ['w', '##i', '##l', '##l'],\n",
       " 'be': ['b', '##e'],\n",
       " 'able': ['a', '##b', '##l', '##e'],\n",
       " 'to': ['t', '##o'],\n",
       " 'understand': ['u',\n",
       "  '##n',\n",
       "  '##d',\n",
       "  '##e',\n",
       "  '##r',\n",
       "  '##s',\n",
       "  '##t',\n",
       "  '##a',\n",
       "  '##n',\n",
       "  '##d'],\n",
       " 'how': ['h', '##o', '##w'],\n",
       " 'they': ['t', '##h', '##e', '##y'],\n",
       " 'are': ['a', '##r', '##e'],\n",
       " 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n",
       " 'and': ['a', '##n', '##d'],\n",
       " 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n",
       " 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = dict()\n",
    "\n",
    "for words in word_freqs:\n",
    "    first_chr = [words[0]]\n",
    "    splits[words] = first_chr\n",
    "    for chr in words[1:]:\n",
    "        letter = \"##\" + chr\n",
    "        splits[words] = splits[words] + [letter]\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we are ready for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computes the score of each pair. We’ll need to use this at each step of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### My bad :(\n",
    "# # def compute_pair_scores(splits):\n",
    "# letter_freqs = dict()\n",
    "# pair_frqs = dict()\n",
    "# score_first = dict()\n",
    "# score_second = dict()\n",
    "# for keys, values in splits.items():\n",
    "#     # print(values)\n",
    "#     for i in range(len(values)-1):\n",
    "#         # print(i)\n",
    "#         # print(f'{len(values)=}')\n",
    "#         pair_chr = (values[i], values[i+1])\n",
    "#         chr = values[i] \n",
    "\n",
    "#         if pair_chr not in pair_frqs:\n",
    "#             pair_frqs[pair_chr] = 1\n",
    "#         else:\n",
    "#             pair_frqs[pair_chr] += 1\n",
    "        \n",
    "#         if chr not in letter_freqs.keys():\n",
    "#             letter_freqs[chr] = 1\n",
    "#         else:\n",
    "#             letter_freqs[chr] += 1\n",
    "        \n",
    "#         if i == len(values)-2:\n",
    "#             # print(f'{i}')\n",
    "#             chr_last = values[i+1]\n",
    "#             if chr_last not in letter_freqs.keys():\n",
    "#                 letter_freqs[chr_last] = 1\n",
    "#             else:\n",
    "#                 letter_freqs[chr_last] += 1\n",
    "\n",
    "    \n",
    "# for keys_freq in pair_frqs.keys():\n",
    "#     for _, values in splits.items():\n",
    "#         for i in range(len(values)-1):\n",
    "#             if keys_freq[0] == values[i] and keys_freq[1] == values[i+1]:\n",
    "#                 if keys_freq not in score_first:\n",
    "#                     score_first[keys_freq] = 1\n",
    "#                 else:\n",
    "#                     score_first[keys_freq] += 1\n",
    "    \n",
    "# for keys in score_first.keys():\n",
    "#         score_second[keys] = score_first[keys] / (letter_freqs[keys[0]] * letter_freqs[keys[1]])\n",
    "\n",
    "# pair_max = \"\"\n",
    "# max_score = None\n",
    "# for keys , values in score_second.items():\n",
    "#     if max_score is None or max_score < values:\n",
    "#         max_score = values\n",
    "#         pair_max = keys[0] + keys[1]\n",
    "\n",
    "# print(pair_max)\n",
    "# print(max_score)\n",
    "# score_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :D\n",
    "def compute_pair_scores(splits):\n",
    "    letter_freq = dict()\n",
    "    pair_freq = dict()\n",
    "    scores = dict()\n",
    "\n",
    "    for words, freq in word_freqs.items():\n",
    "        split = splits[words]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split)-1):\n",
    "            pair = (split[i] , split[i+1])\n",
    "            if pair not in pair_freq.keys():\n",
    "                pair_freq[pair] = freq\n",
    "            else:\n",
    "                pair_freq[pair] += freq\n",
    "            \n",
    "            if split[i] not in letter_freq.keys():\n",
    "                letter_freq[split[i]] = freq\n",
    "            else:\n",
    "                letter_freq[split[i]] += freq\n",
    "        if split[-1] not in letter_freq.keys():\n",
    "            letter_freq[split[-1]] = freq\n",
    "        else:\n",
    "            letter_freq[split[-1]] += freq\n",
    "\n",
    "    for keys, values in pair_freq.items():\n",
    "        if keys not in scores.keys():\n",
    "            scores[keys] = pair_freq[keys] / (letter_freq[keys[0]] * letter_freq[keys[1]])\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h'): 0.125\n",
      "('##h', '##i'): 0.03409090909090909\n",
      "('##i', '##s'): 0.02727272727272727\n",
      "('i', '##s'): 0.1\n",
      "('t', '##h'): 0.03571428571428571\n",
      "('##h', '##e'): 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '##b')\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "#finding pair with best score\n",
    "\n",
    "pair_max = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        max_score = score\n",
    "        pair_max = pair\n",
    "\n",
    "print(pair_max)\n",
    "print(max_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for words in word_freqs:\n",
    "        split = splits[words]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split)-1:\n",
    "            if split[i] == a and split[i+1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a+b \n",
    "                split = split[:i] + [merge] + split[i+2:]\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        splits[words] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', '##o', '##u', '##t']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splits = merge_pair(\"a\", \"##b\", splits)\n",
    "# splits[\"about\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
    "vocab_size = 70\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token= (best_pair[0] + best_pair[1][2:] \n",
    "                if best_pair[1].startswith(\"##\")\n",
    "                else best_pair[0] + best_pair[1])\n",
    "    vocab.append(new_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[UNK]',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " '[MASK]',\n",
       " '##a',\n",
       " '##b',\n",
       " '##c',\n",
       " '##d',\n",
       " '##e',\n",
       " '##f',\n",
       " '##g',\n",
       " '##h',\n",
       " '##i',\n",
       " '##k',\n",
       " '##l',\n",
       " '##m',\n",
       " '##n',\n",
       " '##o',\n",
       " '##p',\n",
       " '##r',\n",
       " '##s',\n",
       " '##t',\n",
       " '##u',\n",
       " '##v',\n",
       " '##w',\n",
       " '##y',\n",
       " '##z',\n",
       " ',',\n",
       " '.',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'w',\n",
       " 'y',\n",
       " 'ab',\n",
       " '##fu',\n",
       " 'Fa',\n",
       " 'Fac',\n",
       " '##ct',\n",
       " '##ful',\n",
       " '##full',\n",
       " '##fully',\n",
       " 'Th',\n",
       " 'ch',\n",
       " '##hm',\n",
       " 'cha',\n",
       " 'chap',\n",
       " 'chapt',\n",
       " '##thm',\n",
       " 'Hu',\n",
       " 'Hug',\n",
       " 'Hugg',\n",
       " 'sh',\n",
       " 'th',\n",
       " 'is',\n",
       " '##thms',\n",
       " '##za',\n",
       " '##zat',\n",
       " '##ut']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we pre-tokenize it, \n",
    "2. split it, \n",
    "3. then apply the tokenization algorithm on each word\n",
    "4. That is, we look for the biggest subword starting at the beginning of the first word and split it, \n",
    "5. then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCITING ALGORITHM :D\n",
    "def encode_word(word_new):\n",
    "    tokens = []\n",
    "    while len(word_new) > 0:\n",
    "        i = len(word_new)\n",
    "        while i > 0 and word_new[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return[\"[UNK]\"]\n",
    "        tokens.append(word_new[:i])\n",
    "        word_new = word_new[i:]\n",
    "        if len(word_new) > 0:\n",
    "            word_new = \"##\" + word_new\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let’s test it on one word that’s in the vocabulary, and another that isn’t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[UNK]',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " '[MASK]',\n",
       " '##a',\n",
       " '##b',\n",
       " '##c',\n",
       " '##d',\n",
       " '##e',\n",
       " '##f',\n",
       " '##g',\n",
       " '##h',\n",
       " '##i',\n",
       " '##k',\n",
       " '##l',\n",
       " '##m',\n",
       " '##n',\n",
       " '##o',\n",
       " '##p',\n",
       " '##r',\n",
       " '##s',\n",
       " '##t',\n",
       " '##u',\n",
       " '##v',\n",
       " '##w',\n",
       " '##y',\n",
       " '##z',\n",
       " ',',\n",
       " '.',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'w',\n",
       " 'y',\n",
       " 'ab',\n",
       " '##fu',\n",
       " 'Fa',\n",
       " 'Fac',\n",
       " '##ct',\n",
       " '##ful',\n",
       " '##full',\n",
       " '##fully',\n",
       " 'Th',\n",
       " 'ch',\n",
       " '##hm',\n",
       " 'cha',\n",
       " 'chap',\n",
       " 'chapt',\n",
       " '##thm',\n",
       " 'Hu',\n",
       " 'Hug',\n",
       " 'Hugg',\n",
       " 'sh',\n",
       " 'th',\n",
       " 'is',\n",
       " '##thms',\n",
       " '##za',\n",
       " '##zat',\n",
       " '##ut']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let’s write a function that tokenizes a text:D KHAFAN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th',\n",
       " '##i',\n",
       " '##s',\n",
       " 'is',\n",
       " 'th',\n",
       " '##e',\n",
       " 'Hugg',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " 'Fac',\n",
       " '##e',\n",
       " 'c',\n",
       " '##o',\n",
       " '##u',\n",
       " '##r',\n",
       " '##s',\n",
       " '##e',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so based on training dataset we create and update vocab list that we can tokenize text and corpus :))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
