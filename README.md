This is a good site to understand attention heads and how they work visually. It is provided by PyTorch:
* https://pytorch.org/blog/inside-the-matrix/

---
Here is the link to my slides on Transformers: https://www.canva.com/design/DAGTx-yiKCo/nzQ_a-AsFNsqq30ac9tLlQ/edit?utm_content=DAGTx-yiKCo&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton
