{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qxy699/hugging_face/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data_files = {\"train\": \"/home/qxy699/Data/WHO_representative_random/who_dataset_training.csv\", \n",
    "              \"test\": \"/home/qxy699/Data/WHO_representative_random/who_dataset_test.csv\"}\n",
    "# \\t is the tab character in Python\n",
    "sars_dataset = load_dataset(\"csv\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = sars_dataset[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "vocab_size = how many unique tokens the tokenizer should recognize and \n",
    "maintain in its dictionary.  a larger vocabulary can capture more specific or rare words, \n",
    "potentially improving the model's ability to understand nuanced text, but it may also \n",
    "require more memory and computational resources. Conversely, a smaller vocabulary size \n",
    "might lead to a more generalized model that is efficient but less capable of understanding \n",
    "specific or rare terms, as these will be grouped into more general tokens or categorized \n",
    "as unknown.\n",
    "'''\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'c',\n",
       " 'ga',\n",
       " 'att',\n",
       " 'cc',\n",
       " 'gg',\n",
       " 'at',\n",
       " 'g',\n",
       " 'cca',\n",
       " 'at',\n",
       " 'c',\n",
       " 'gat',\n",
       " 'c',\n",
       " 'gg',\n",
       " 't',\n",
       " 'ac',\n",
       " 'ga',\n",
       " 'at',\n",
       " 'gt',\n",
       " 'cat',\n",
       " 'gat',\n",
       " 'g',\n",
       " 'cat']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''atcgaattccggatgccaatcgatcggtacgaatgtcatgatgcat'''\n",
    "\n",
    "old_tokens = old_tokenizer.tokenize(example)\n",
    "old_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atcg',\n",
       " 'aattcc',\n",
       " 'gg',\n",
       " 'atgcc',\n",
       " 'aatcg',\n",
       " 'atcgg',\n",
       " 'tacg',\n",
       " 'aatgtc',\n",
       " 'atg',\n",
       " 'atgc',\n",
       " 'at']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''atcgaattccggatgccaatcgatcggtacgaatgtcatgatgcat'''\n",
    "\n",
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-SARS-tokenizer/tokenizer_config.json',\n",
       " 'code-SARS-tokenizer/special_tokens_map.json',\n",
       " 'code-SARS-tokenizer/vocab.json',\n",
       " 'code-SARS-tokenizer/merges.txt',\n",
       " 'code-SARS-tokenizer/added_tokens.json',\n",
       " 'code-SARS-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-SARS-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load again\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"code-SARS-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atcgaa\n",
      "ttccgg\n",
      "atgcca\n",
      "atcgat\n",
      "cggta cgaat\n",
      "gaatg tcatga\n",
      "catga tgcat\n"
     ]
    }
   ],
   "source": [
    "sequence = \"atcgaa\", \"ttccgg\", \"atgcca\", \"atcgat\", \"cggta cgaatg tcatga tgcat\"\n",
    "inputs = old_tokenizer(\n",
    "    sequence, \n",
    "    truncation=True, \n",
    "    max_length=6, stride=2,\n",
    "    return_overflowing_tokens=True\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(old_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[265, 66, 4908, 64], [926, 535, 1130], [265, 70, 13227], [265, 66, 41268], [66, 1130, 8326, 269, 4908, 265], [4908, 265, 70, 256, 9246, 4908], [9246, 4908, 256, 70, 9246]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 1, 2, 3, 4, 4, 4]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atcgaa\n",
      "ttccgg\n",
      "atgcca\n",
      "atcgat\n",
      "cggta cgaatg tcatga tgcat\n"
     ]
    }
   ],
   "source": [
    "sequence = \"atcgaa\", \"ttccgg\", \"atgcca\", \"atcgat\", \"cggta cgaatg tcatga tgcat\"\n",
    "inputs = old_tokenizer(\n",
    "    sequence, \n",
    "    truncation=True, \n",
    "    # max_length=6, stride=2,\n",
    "    return_overflowing_tokens=True\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(old_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[265, 66, 4908, 64], [926, 535, 1130], [265, 70, 13227], [265, 66, 41268], [66, 1130, 8326, 269, 4908, 265, 70, 256, 9246, 4908, 256, 70, 9246]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
